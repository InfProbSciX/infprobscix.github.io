---
layout: posts
title:  "Likelihoods in High Dimensions"
date:   2018-07-05 00:00:00 +0100
categories: stats
entries_layout: grid
---


## Volumes in High Dimensions

A lot of probability theory uses the idea of volumes. For example, the probability and expectation are usually defined as:

$$ \mathbb P_{\mu} (\mathcal A) = \int_A d \mu \;\;\; ; \;\;\; \mathbb E_{\mu} (f(X)) = \int_{\chi} f(x) d \mu(x) $$

... where the differential element is usually a volume, \\( d \mu(x) = p(x) dx_1 ... dx_d\\), and p is a density ...

... but volumes are really weird in higher dimensions.

The main idea is that our ideas of _distances_ no longer make sense. Consider the following: I simulated a bunch of random normal points in many dimensions and plotted the average ratio of the distance to nearest point to the distance to farthest point:

<center> <img style="max-width: 500px; height: auto;" src = "/images/intro_f.png"> </center>

This ratio tends to one, implying that the nearest and farthest points are very "close" together / all points "look alike" <sup>[2]</sup>.

On the same subject, consider a cube in higher dimensions:
<center> <img style="max-width: 400px; height: auto;" src = "/images/cubes.png"> </center>

Arguing as in <sup>[5]</sup>, the volume of any of these hypercubes is always 1 (_side x side x side ... = 1 x 1 x 1 ..._). Now imagine a 100-dimensional hypercube where every side has length 0.99 sitting inside another 100-dimensional hypercube with length 1 along every side. The area of the smaller cube is _just 37% of the larger cube_ even though the length of every side is 99% of the length of the bigger cube's sides.

The way this is interpret is that ***most of the area of a hypercube is concentrated on a thin shell, near the surface***<sup>[2, 5]</sup>.

Spheres seem to do weird things too in higher dimensions, in fact, the volume of a hyper-sphere begins to _decrease after dimension 5_ ([Wikipedia Source](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)).

So, if we inscribe a sphere within a cube as such:

<center> <img style="max-width: 500px; height: auto;" src = "/images/sph_insc.png"> </center>

... we notice that the ratio of the volume of the cube (1) to the volume enclosed by the sphere goes from 1 to 0.

All of this discussion has a huge implication on computational statistics, as all of our sampling methods rely on a density defined on some volume. Moreover, this has many other areas of effect, for example, in optimization (all points look the same so it's hard to propose a new point) and in cluster analysis (e.g. the KNN would fail as the closest point is as far away as the farthest point).

## Effect on Likelihoods

This discussion leads to one very interesting observation: a probability is not a good measure of _typicality_ outside of the first dimension.

Consider the following: the mode of a normal is at zero. In one dimension, if I draw from a normal, the most reasonable guess at what the draw is, is zero. In high dimensions, this isn't the case - if I draw from a 1000-dimensional normal, it's ridiculously unlikely that I'd pick a vector that (0, 0, 0, ...); a more reasonable guess would be "a vector of points where the mean radius is ye much".

A very good analogy I've read on CrossValidated <sup>[1]</sup> is this: let's say that the weights, heights, eyesight, athleticism, intelligence, ... of people is normally distributed. If I picked a random guy on the street and asked him his weight, it would be reasonable to expect that it'd be around the average. Instead, if I asked the person about their height, weight, eyesight ..., it's very unlikely to meet a person who's average at everything. I'm a bit more heavy, averagely tall, more books-ey ... than other people, for example.

Now consider the likelihood of an isolated repeated experiment:

$$L(\theta | x) = \prod_i^n p(x_i|\theta)$$

This is a function of many random variables (which collectively have a dimension n). This will have it's own sampling distribution, and the position where it's maxmized w.r.t. any of the x's isn't a typical value of the likelihood at all.

Example: 

{%highlight ruby%}

hist_fancy(sapply(1:10000, function(i) sum(log(dnorm(rnorm(100))))),
           main = "Typical Values of the Loglikelihood", xlab = "ll")

{% endhighlight %}

<center> <img style="max-width: 500px; height: auto;" src = "/images/like_typ.png"> </center>

The maximum possible value of the loglikelihood in this case is -91.9, although it's always much lower than that. Loglikelihood values around -140 _for this experiment_ are _typical_.

We call values that lie in this region of typicality the ***typical set*** and it's geometry is expected to be weird. From the other results, we expect it to be a thin layer of mass lying somewhere in a high dimensional space.

Interpolating between any two values of the typical set is rather unlikely to result in another typical sample; this is apparently why optimizers and "ensemble methods" (ones which initialize a bunch of points and move them around till they look like a sample from a density) are a bad way to sample and fail in higher dimensional spaces. This is why HMC is said to be so powerful; to sample from an arbitrary density, we can't use grid or ensemble based methods, so a logical alternative is to start with a sample and obtain another sample, which leads logically to the idea of Markov Chain Monte Carlo, at which the Hamiltonian way is rather efficient.

On the topic of typical sets, Michael Betancourt <sup>[7]</sup> has an even more interesting example; instead of sampling, Betancourt first reparameterizes a target density and integrates out all of the nuisance parameters to obtain a marginal distribution of a set of normals w.r.t. their radius: (this is taken directly from the booklet "Logic, Probability, and Bayesian Inference" by Betancourt & the Stan Dev Team <sup>[7]</sup>)

---

> Consider a probability distribution over the \\(D\\)-dimensional real 
numbers represented by a product of Gaussian probability density 
functions,

$$
\begin{equation*}
p_{\Theta} \! \left( \theta \right) = 
\prod_{d = 1}^{D} \frac{1}{\sqrt{2 \pi \sigma^{2} } }
\exp \! \left( - \frac{\theta_{d}^{2}}{2 \sigma^{2}} \right),
\end{equation*}
$$

> with the corresponding rectangular Lebesgue measure,

$$
\begin{equation*}
\mathrm{d}^{n} \theta = \prod_{d = 1}^{D} \mathrm{d} \theta_{d}.
\end{equation*}
$$

> Because both the probability density function and Lebesgue
measure are invariant to rotations, it is easier to analyze this
distribution with hyper-spherical coordinates.  In hyper-spherical
coordinates the original probability density function appears as

$$
\begin{equation*}
p_{\Theta} \! \left( r, \phi_{1}, \ldots, \phi_{D - 1} \right) = 
\left( \frac{1}{2 \pi \sigma^{2} } \right)^{\frac{D}{2}}
\exp \! \left( - \frac{r^{2}}{2 \sigma^{2}} \right),
\end{equation*}
$$

> and with respect to the hyper-spherical Lebesgue measure
the rectangular Lebesgue measure appears as

$$
\begin{equation*}
\mathrm{d}^{n} \theta =
r^{D - 1} \prod_{d = 1}^{D - 1} \sin^{D - d - 1} \! \left( \phi_{d} \right) 
\cdot \mathrm{d} r \prod_{d = 1}^{D} \mathrm{d} \phi_{d}.
\end{equation*}
$$

(note the use of the Jacobian here)

> From the hyper-spherical perspective, our original probability
density function concentrates around the mode and rapidly
decays with increasing radius, \\(r\\).  The rectangular Lebesgue 
measure, however, becomes increasingly larger at higher
radius where there is more and more volume over which to 
integrate.

<center> <img src = "/images/bet_a.png"> </center>

_img source: Betancourt et al. <sup>[7]</sup>_

> To see where the probability distribution concentrates radially
we pushforward the probabiltiy density function,

$$
\begin{equation*}
p_{\Phi} \! \left( r, \phi_{1}, \ldots, \phi_{D - 1} \right)
=
\left( \frac{1}{2 \pi \sigma^{2} } \right)^{\frac{D}{2}}
\exp \! \left( - \frac{r^{2}}{2 \sigma^{2}} \right)
r^{D - 1} \prod_{d = 1}^{D - 1} \sin^{D - d - 1} \! \left( \phi_{d} \right),
\end{equation*}
$$

> and then marginalize out the hyperspherical angles analytically
to give the radial probability density function,

$$
\begin{equation*}
p_{R} \! \left( r \right) = 
\left( 2 \sigma^{2} \right)^{-\frac{D}{2} }
\frac{ 2 }{ \Gamma \! \left( \frac{D}{2} \right) }
r^{D - 1} \exp \! \left( - \frac{r^{2}}{2 \sigma^{2}} \right),
\end{equation*}
$$

> which is exactly the scaled \\(\chi\\) distribution.  In particular, for
large \\(D\\) all of the probability concentrates in a neighborhood
around \\(r \approx \sigma \sqrt{D}\\) with width approximately equal
to \\(\sigma / \sqrt{2}\\).  In other words, almost all of our target probability 
can be found in a thin shell at \\(r = \sigma \sqrt{D}\\) and the relative 
width of that shell decreases as we add more and more dimensions.

<center> <img src = "/images/bet_b.png"> </center>

_img source: Betancourt et al. <sup>[7]</sup>_

---

## Other comments

The idea I was pursuing with the sampling distribution of likelihoods was the thought that it would lead to MacKay's graph about parsimony and model selection but instead of an evidence-based approach, this would be a likelihood-based approach (invariance of the prior). This has no mathematical reasoning behind it, and yet it seems to be quite similar to (but very crude in the light of) information theoretic approaches or the posterior predictive density.

<center> <img src = "/images/mk_occ_raz.png"> </center>

_img source: MacKay <sup>[6]</sup>_

<br>

## Appendix: Code

My diagrams were made using Inkscape.

<details>
<summary> R Code for Ratio of Nearest/Farthest Distances Image </summary>

{%highlight ruby%}

library(mvtnorm); library(ggplot2)

dim_dist <- function(d, samp_size = 10, ...){
	samp <- rmvnorm(samp_size, mean = rep(0, d), sigma = diag(1, d, d))
	dists <- as.matrix(dist(samp, diag = T, upper = T, ...))
	return(mean(sapply(1:samp_size, function(i) min(dists[-i, i])/max(dists[-i, i]))))
}

qplot(x = 1:250,
	  y = sapply(1:250, function(d) dim_dist(d)),
	  main = "Euclidean Distance in High Dimensions",
	  ylab = "Average Ratio of Nearest to Furthest Point",
	  xlab = "Dimension", color = I("dark gray"), ylim = c(0, 1)) +
	  geom_hline(aes(yintercept = 1))

{% endhighlight %}

</details> <br>

## Bibliography

_Note: The content here looks very similar to <sup>[3]</sup> by accident - I found these sources after arriving independently at some of these results, but you are invited to check them out._.

1. Why is Euclidean distance not a good metric in high dimensions?
 * _Accessible on [Cross Validated](https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions)_.
2. A Few Useful Things to Know about Machine Learning
 * Domingos P., 2012. _Accessible at [washington.edu](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)_.
3. Ensemble Methods are Doomed to Fail in High Dimensions
 * Carpenter B., 2017. _Accessible at [andrewgelman.com](http://andrewgelman.com/2017/03/15/ensemble-methods-doomed-fail-high-dimensions/)_.
4. Michael Betancourt: Speaking (accessible at: [betanalpha](https://betanalpha.github.io/speaking/)).
 * This talk: [Efficient Bayesian inference with Hamiltonian Monte Carlo](https://www.youtube.com/watch?v=pHsuIaPbNbY) is ***highly*** recommended.
5. High-Dimensional Spaces Are Counterintuitive, Part 2
 * Lippert E., 2005. _Accessible at [Microsoft MSDN Blogs](https://blogs.msdn.microsoft.com/ericlippert/2005/05/13/high-dimensional-spaces-are-counterintuitive-part-two/)_.
6. Information Theory, Inference, and Learning Algorithms (pg. 344)
 * MacKay D.J.C., 2005. _Accessible at [inference.org.uk](http://www.inference.org.uk/itprnn/book.pdf)_.
7. ***Logic, Probability, and Bayesian Inference***: ...
 * Stan Dev Team, 2016. _Accessible on [GitHub/betanalpha/stan_intro](https://github.com/betanalpha/stan_intro/blob/master/stan_intro.pdf)_. This is a _highly_ recommended read.
