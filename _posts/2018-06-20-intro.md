---
layout: posts
title:  "Introduction to the Study of Data"
date:   2018-06-20 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

When dealing with data, I mostly find myself in one of three scenarios:
1. There's no randomness in the data: Situations like this are quite rare, but when they do arise, dealing with them is quite easy; all we'd need to do when confronting such datasets is _**build a set of rules**_ by studying the data.
	*  Historically, this is how a lot of physical laws were discovered, particularly when the phenomenon were so spectacular that they swamped out any little noise present in observations.

2. There's some randomness in the data: There's some uncertainty present in the data, but certain _**frequencies**_ emerge (or are expected to emerge) when studying the data. For example, if I sit still and toss a coin whose mass is distributed evenly / uniformly throughout the coin, I might observe that after many throws, the number of heads comes out to be roughly the same as the number of tails. Where such frequencies seem to exist, probability theory can be used to _**represent**_ those things which seem random but have the peculiar property that their frequencies are roughly predictable. This process of representing random things and trying to understand how they behave is statistics.

3. There's a lot of randomness in the data: It is not at all obvious that there should exist random things with predictable frequencies. If I'm on a moving bus when I'm tossing the coin, I might see that every time the bus banks left, I tend to get more heads than tails and vice versa. So, the random thing here - the number of heads compared with tails - depends on the journey and route I'm taking, and the frequencies of heads is not going to be predictable at all. In cases such as this however, the reason for unpredictability is usually the complexity of the thing we're looking at. If we simplify the problem (i.e. _**consider limiting cases**_), we often end up in scenario 2.

The fact that situation ones are rare and situation threes can be decomposed into situation twos is what makes statistics so important.

Note that, historically, "uncertainty" just means ignorance or complexity. In the coin tossing example, we can predict exactly if the coin ends up being a head or a tail, but often, it's too difficult or unimportant to understand underlying dynamics, and we find ourselves resorting to statistical methods.

Note that, in point 2, I do not claim that the existence of 'stable' or predictable frequencies _defines_ the study of statistics - it is merely an idea and an assumption that we make (and justify) while working with data. One runs into circularity problems really quickly if they try to define statistics in terms of "limiting" frequencies or with other similar ideas.

In fact, this line of reasoning needn't only hold for the idea of frequencies. Consider the plausibility of someone on a court stand being guilty of a crime. Although the jury may never know the truth about the person's extent of culpability, they can reason that: if observables are represented as probabilistic variables, given a causal chain of events, the observables would probably never have happened if not for the person's involvement, thus increasing the likelihood of their guilt. This shows that "subjective" views of probability have merit and fundamentally all forms of measuring plausibility may be the same underlying principle.

## Probability Theory

Probability theory gives us a foundation to define and work with random things whose frequencies can be fixed.

(The SEP<sup>[4]</sup>, Jaynes' book<sup>[3]</sup> and similar sources have extremely well-framed presentations on this topic. Below is an extremely short summarization of that information.)

A probability can be interpreted in a number of ways, such as <sup>[4]</sup>:
 * the "fraction" of times an event happens / is "expected" to happen relative to the "fraction" of times it doesn't,
 * the "degree of belief" that a certain event will occur,
 * the "physical disposition" of a situation to yield a particular outcome, etc ...

The key point is that, although all such can be viewed as logical or part of the same overarching _idea_, it's very difficult to _define_ probabilities using such arguments.

There have been two paradigm-defining attempts to define probability.

The first was the description of the laws of probability by A. N. Kolmogorov<sup>[5]</sup>. These laws are as follows<sup>[4, paraphrased by me]</sup>:

Let \\(\Omega\\) be a set of things that can happen and \\(\Sigma\\) is a bunch of interesting questions we can ask. Then, we let \\(\mathcal P\\) be a function called a probability function that takes in interesting questions and spits out a number. \\(\mathcal P\\) obeys the following rules:
 1. \\(\mathcal P(interesting \; question)\\) is either 0 or more than 0.
 2. \\(\mathcal P(\omega)\\) is 1 (so the probability that something - _anything_ - from \\(\Omega\\) happens in 1).
 3. If two things can't both happen, then \\(P(A\;or\;B\;happening) = P(A) + P(B)\\)

Note that this definition doesn't rely on any interpretation. Probability laws are applicable to any mathematical or physical system that obeys these rules. As I understand, there are things in statistical mechanics that obey the rules stated above which have no relation to "probability" in any way whatsoever but the theory still holds.

---
### <span style="color:orange "> Technical Interlude </span>

Feel free to skip this section if you've got no interest in the math. Formally, we need measure theory to properly define probability as Kolmogorov did and although it is so abstract, if you try to define probability, you'll find yourself rediscovering these ideas presented below.

First, let \\(\Omega\\) be a "sample space" - these are events that are interesting (e.g. \\(\Omega = \\{"head", "tail"\\} \\) is a sample space of an experiment which involves tossing a coin once).

Before we get to probability, note that there are a bunch of interesting question we can form with such a set. A probability needs to be _consistent_ throughout all of these interesting questions.

For example, I might ask "What is the probability of heads?" but this can also be answered by asking "What is the probability of tails?" because I can ask "What is the probability that I get either a head or a tail?" and if we set this to 1 (point 2 above), we can say that the "Prob of Head" + "Prob of Tail" = 1 (using point 3) and calculate "Prob of Head" by subtracting "Prob of Tail" from 1.

Note what the probability had to do here: for every event \\(A\\) our probability function knows, the probability function also knows \\(not A\\) means (i.e. it can ). Furthermore, for any two events \\( A \; \& \;B \\) that the probability function can handle, it must also be able to handle \\(A or B\\).

Another example: say that you're tossing a six-sided die (\\(\Omega = \\{1, 2, 3, 4, 5, 6\\} \\)). If I code up a function called probability into a computer and if I define it for events 2, 3 & 6 as \\(\mathcal P(2) = \mathcal P(3) = \mathcal P(6) = 1/6 \\), it should also be able to process questions like \\(\mathcal P(2 or 3) = 2/6 \\) and \\(\mathcal P(not 2) = 5/6 \\).

Mathematically, we say that the set of things \\(\mathcal P\\) needs to be able to work with is _closed under complements_ and also _closed under unions_.

We call such a set of interesting questions a _**\\(\sigma\\)-algebra**_ or a _**\\(\sigma\\)-field**_ that's defined on elements of \\(\Omega\\). The pair of these two sets (\\(\Omega\\), \\(\Sigma\\)) is called a measurable space<sup>[3]</sup> and it is on such sets we define a measure with the normalization, non-negativity and finite additivity conditions and call it a probability measure<sup>[4]</sup>. The triple \\((\Omega, \Sigma, \mathcal P)\\) is called a probability space. The key difference between a measure space and a probability space is that a probability space is endowed with a measure which normalizes to 1<sup>[3]</sup>.

For convenience, we usually work with functions called _**random variables**_ \\(X:\Omega \rightarrow \mathbb R \\) which are defined on measurable spaces. Sometimes, we replace \\(\mathbb R\\) with some other set and call the function a "random element" instead. We do this mainly for convenience e.g. \\( \\{"head", "tail" \\} \rightarrow \\{ 0, 1 \\} \\).

Measurable spaces are a big deal because you'd run into major problems defining measures on non-measurable spaces. Famous (and _mind-blowing_) examples include the [Vitali Set](https://en.wikipedia.org/wiki/Vitali_set) and the [Banach-Tarski Paradox](https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox). It's profoundly amazing that the construction of these sets relies on the axiom of choice; most mathematicians are comfortable accepting the existence of sets which have no measure than abandoning the axiom of choice. Here's a [cool video](https://www.youtube.com/watch?v=hcRZadc5KpI) on that by PBS Infinite Series & Kelsey Houston-Edwards.

---

The other formalization is due to Cox presented by Jaynes in <sup>[2]</sup> (please refer to this for a full argument). The argument presented is that the Kolmogorov rules can derive by three logical arguments:
 1. "Degrees of plausibility are represented by real numbers".
 2. "Common sense"
 3. "If a conclusion can be reasoned out in more than one way, every possible way should lead to the same result."


Much of probability theory can be derived from the basic axioms put forward by Kolmogorov. This includes the famous Bayes' Theorem:

$$ \mathcal P(B | A) = \frac{\mathcal P(A \; and \; B)}{\mathcal P(A)} $$

This construct also allows us to define _distributions_. For example, a discrete-valued distribution may be defined in terms of the probabilities associated with particular values:

$$ X = \{ z_i \; with \; probability \; p(z_i) $$

Where \\(p\\) is a function that maps integers to probabilities. It  is called a probability mass function. We can extend these to continuous spaces by letting \\(\epsilon p(x)\\) represent the probability of finding a point between \\(x - \epsilon, x + \epsilon\\). This is called a probability density function.

Probability theory gives us very nice interpretations for such probability functions. Particular _forms_ of probability mass / density functions have very particular _meanings_ i.e. they emerge naturally from particular sets of assumptions.

For example:

$$ p(0) = 1 - r \; \leftrightarrow \; p(1) = r $$

... arises naturally in experiments with a chance of success equal to r and a chance of failure equal to \\(1 - r\\). This is called a Bernoulli distribution.

$$ p(x) = \lambda e^{-\lambda x} $$

... arises naturally in processes with random waiting times between events where the only known information about the waiting times is that the average waiting time is \\(1/\lambda\\). This is called an exponential distribution.

$$ p(x) = \frac{1}{\sqrt{ 2 \pi }} e^{- \frac{x^2}{2}} $$

... arises naturally in the estimation of averages (shifted and scaled appropriately). This is called a Gaussian or a Normal distribution.

The "r" and the "p" are numbers that produce slightly differently-shaped distributions based on the data set. In fact, estimating these numbers, called parameters, using data is a huge part of statistical inference.

---
### <span style="color:orange "> Technical Interlude </span>

There's an incredible result that shows that many probability distributions are the result of maximizing the entropy (the randomness in some sense) of an arbitrary probability distribution subject to a set of constraints.

If you know the probability of success for a particular process and nothing else, the best you can do is to represent the process using a Bernoulli distribution.

If you know the mean and variance of a particular process and nothing else, the best you can do is to represent it using a normal distribution.

The exponential family is in fact a family of parameterized maximum entropic distributions (i.e. they arise in maximum-entropy conditions with different constraints and have parameters which usually have a very deep physical meaning). It is also the only family of distributions where the dimension of numbers required to estimate them (i.e. the number of sufficient statistics) _**doesn't grow**_ with the number of samples available (this provides further intuition regarding their randomness - they're so random that you can't do any better).

Sufficiency in statistics is the idea that sometimes, you just need a few numbers to completely charecterize a distribution. For example, if you know the sample mean and variance of a normal-looking dataset, you don't need to keep the dataset as the data doesn't give you any more information that's not already present in the mean and variance.

---

Now that we've established a way to deal with things whose frequencies remain roughly stable, we can apply this theory to data.

## Statistics

First, when we obtain data, a foundational step in data processing is to look at so called "[descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics)" which is an attempt to describe data on hand, either through summary statistics or through visualizations:

<center> <img src = "/images/intro_a.png"> </center>
<sup> [a] </sup>

The objective of studying data is usually to try and understand:
 * the structure, composition and the interrelationships evident in the data or
 * how certain parts of the data explain / predict other parts of the data

I personally dislike the labeling that's done here ([Generative](https://en.wikipedia.org/wiki/Generative_model) vs [Discriminatory](https://en.wikipedia.org/wiki/Discriminative_model)) but more on that later.

This blog is mainly focused on statistical inference:

<div style="text-align:center"> "Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed." <sup> [1, pg. 4] </sup> </div> <br>

We usually do this by constructing a statistical model, the aim of which is to build a plausible probabilistic representation capable of reproducing (joint) frequencies and other features evident in the data.

### Non-Parametric Inference

(Write about the bootstrap, how it is such a good estimator of stuff. Also, it can be thought of as sampling n points from the eCDF.)

### Parametric Inference

This usually involves writing down a probabilistic model w.r.t. some parameters and trying to estimate those parameters.

A widely accepted way to do this is to treat the parameters of a model as random variables and construct probability distributions for them using the Bayes' rule:

$$ p_{\mathcal M} (\theta | {\bf x}) = \frac{p_{\mathcal M} ({\bf x} | \theta) p(\theta)}{p_{\mathcal M} ({\bf x})} \leftrightarrow posterior = \frac{likelihood * prior}{marginal \; likelihood} $$

Another way to estimate parameters, if the parameters are known to be singular points, is to ***maximize*** the likelihood function. The likelihood function, and particularly, the _log_-likelihood function have some remarkable properties, and it isn't obvious why maximizing the likelihood function _works_. The maximum likelihood estimate can be shown to be consistent (i.e. it will pick the right estimate on average) and in many cases, it achieves the lowest possible variance ***any*** "unbiased" estimator can have.

The likelihood is an amazing, non-trivial construct. It can be defined as the model joint distribution evaluated at the data. In fact, minimizing the KL-divergence between the hypothesized distribution and the data (in some sense) is equivalent to maximum likelihood estimation.

This can be seen as a special case of a more general idea:

***Claim:***

_Let \\(Y\\) be a multivariate random variable with mass or density \\(p\\). Further, let_ \\( p_{\mathcal M}\\) _be a hypothesis density such that_ \\(p \sim p_{\mathcal M}\\). _Then:_

$$\mathbb E_p \left( ln \; p(Y) \right) \geq \mathbb E_p \left( ln \; p_{\mathcal M} (Y) \right) $$

_with equality only when_ \\(p = p_{\mathcal M}\\).

***Proof:***

We need to show that the following expression is greater than or equal to 0, or equivalently, negative of that expression, less than or equal to zero:

$$ - \left( \mathbb E_p \left( ln \; p_{\mathcal M} (Y) \right) - \mathbb E_p \left( ln \; p(Y) \right) \right) = \mathbb E_p \left( ln \; \frac{p_{\mathcal M} (Y)}{p(Y)} \right) $$

$$ \overset{Jensen}{\leq} ln \; \mathbb E_p = \; ln \int \frac{p_{\mathcal M}}{p} dp = \; ln \int p_{\mathcal M} dy = \;ln\; 1  = 0 \tag* {$\blacksquare$} $$

Choosing a \\(p_{\mathcal M}\\) to minimize the above is equivalent to maximum likelihood estimation, because the first term doesn't depend on the model, and maximizing the second term is equivalent to making the likelihood as large as possible.

The likelihood doesn't share this result (graphically, this is because the mean is not a robust estimator of the "center" of a distribution and the likelihood is spiked near 0 with a long positive tail; the sampling distribution for the log-likelihood on the other hand, looks nice and its mean is asymptotically normal which would mean that the likelihood is asymptotically log-normal - this may mean that you'd be able to do some kind of validation using these facts).

<center> <img style="max-width: 500px; height: auto;" src="/images/intro_e.png"> </center>

The sampling distributions for the likelihood and log-likelihoods clearly show that the likelihood also, as any other function of a random sample, has a well-defined sampling distribution.

It's interesting that although the likelihood turns out to be concave w.r.t. the parameter at most times, it doesn't account for the correctness of a sample; i.e. a likelihood is not maximized at a representative sample of the population. A simple illustration is the fact that the product of exponential likelihoods at 0 will _**always**_ be higher than the product of likelihoods at a random sample _of that exponential_. So, perhaps model validation using a sampling distribution of likelihoods may be a good idea.

I'm not sure how much value the likelihood offers in terms of favoring more plausible models over highly complex overfit ones simply because of the fact mentioned above. Many use the evidence / marginal likelihood for model comparisons, often choosing the model with the highest evidence - but the evidence is often too sensitive to the prior (not to mention, very difficult to calculate), and seems to be just a weighted average of the likelihood function). Information criteria are chosen instead as they convey the same information as the evidence. Then again, information criteria are just bias corrected log-likelihood terms and perhaps offer no more information than the log-likelihood itself.

For more insight into how the likelihood works, and the extent of its limitations, consider the following scenario: I draw many samples from a Binomial(5, 0.5) and fit two models, a Binomial(5, MLE) and a BetaBinomial(5, MME \\(\alpha\\), MME \\(\beta\\)). Note that the Beta Binomial can account for more variance than the Binomial and has an extra parameter. The KL Divergence between the empirical mass of the sample and the best fit beta-binomial mass is often higher than just the binomial, so in a sense, model checking through LD-divergence is not such a bad idea (note that I split my data into two sets, one to calculate the empirical mass and the other to estimate the best fit parameters, which is a form of cross-validation). The comparison of log-likelihoods also displays this information.

<center> <img src="/images/intro_c.png"> </center>

### The Loss Function

In machine learning, in most cases, one simply builds a function that depends on variables in the data and minimizes a (sometimes arbitrarily constructed) metric called the loss function. For example, in simple linear regression, the sum of squared errors is usually taken to be the loss associated with a particular set of intercept and slope parameters.

The danger in using such approaches is very subtle and unintuitive; as the loss function is a function that depends only on the parameters, and given that the objective after selecting a machine learning model is always choosing a set parameters, the ***loss function does the job of a likelihood function***. Hence, by changing the loss function, one often ***changes the model structure***.

Consider for example, regularization in linear regression, where a penalty term is added to the loss function to motivate it to pick smaller values of a parameter and hence only select parameters which are really significant in the model.

The model is given by:

$$ \widehat Y = \underline \beta^T {\bf X} + \underline \epsilon \;\; | \;\; \underline \epsilon \sim N({\bf 0}, \sigma^2 {\bf I}) $$

$$ \Rightarrow loss = ll(\underline \beta) = \frac{1}{2} \sum_i^n (y_i - \underline \beta^T \underline x)^2 / \sigma^2 - g(\sigma)$$

This is because \\(f_Z(x) = \frac{exp \left( -\frac{x^2}{2} \right)}{\sqrt{2\pi\sigma^2}}\\). The \\(\sigma\\) disappears during differentiation/optimization. To penalize small (insignificant) values of \\( \beta \\), it is common in machine learning to add a penalty of:

$$ k || \underline \beta ||^p $$

Sometimes, choosing a p of 2 makes optimization easy (because gradient information becomes available). Choosing a p of 1 has its own advantages in relatively higher dimensional spaces.

The choice of p, rather unintuitively, has a ***massive*** effect on the model that you fit; choosing a p of 1 (LASSO regression) causes parameters to be zero for a long time and suddenly become positive as the value of k changes (i.e. "selection") and the values of most/all parameters are artificially depressed (i.e. "shrinkage"). Selecting a p of 2 does just depression and the parameters are never exactly 0, but are close to it (i.e. no "selection").

The reason for why can be seen by interpreting the penalty term as a constraint (try expressing the gradient of the loss in a Lagrangian form):

$$ || \underline \beta ||^p < h(k) $$

The paths traced out as k is relaxed is shown below:

<center> <figure>
	<img style="max-width: 400px; height: auto;" src="/images/intro_b.png"> 
	<img style="max-width: 350px; height: auto;" src="/images/intro_d.png"> 
	<figcaption> p = 1 (LASSO) & p = 2 (Ridge) </figcaption>
</figure> </center>

There's also a ***statistical interpretation*** for this; notice that we're adding absolute \\(\beta\\) terms or squared \\(\beta\\) terms to the log-likelihood - these correspond to centered, independent multivariate Laplace (double exponential) and normal distributions! We're essentially just adding priors to our regression model! This makes all the difference if the data doesn't overwhelm prior information.

## Appendix: Code for Reproducing Diagrams

<details>
<summary> a: R Code for making the descriptive statistics image. </summary>

{%highlight ruby%}

library(ggplot2)
library(gridExtra)

data(iris)
data(faithful)

z <- mvtnorm::rmvnorm(10000, c(0, 0), matrix(c(1, 0.5, 0.5, 1), 2, 2))

df <- data.frame("Location" = qgamma(ppoints(10000), 2, 1),
				 "Spread1" = qnorm(ppoints(10000), 0, 1),
				 "Spread2" = qnorm(ppoints(10000), 0, 2),
				 "Kurt1" = qt(ppoints(10000), 2),
				 "Kurt2" = qt(ppoints(10000), 100),
				 "Norm1" = z[, 1],
				 "Norm2" = z[, 2])

plots <- list(
	ggplot(df, aes(x = Location)) +
		geom_histogram(alpha = 0.7, fill = "green", bins = 100) + xlim(c(0, 6)) +
		geom_vline(xintercept = mean(df$Location), lty = 2, alpha = 0.55) + 
		geom_text(aes(x = mean(df$Location) + 0.1, label = "Mean", y = 200), angle = 90, alpha = 0.7) +
		geom_vline(xintercept = median(df$Location), lty = 2, alpha = 0.7) + 
		geom_text(aes(x = median(df$Location) + 0.1, label = "Median", y = 200), angle = 90, alpha = 0.7) +
		geom_vline(xintercept = 1, lty = 2, alpha = 0.7) +
		geom_text(aes(x = 1.1, label = "Mode", y = 200), angle = 90, alpha = 0.7) +
		geom_curve(aes(x = 5, xend = 6, y = 25, yend = 14), arrow = arrow(length = unit(0.25, units = "cm")), curvature = 0.1) + 
		geom_text(aes(x = 5.5, label = "Skew (+)", y = 23), angle = -20) +
		labs(y = "Frequency", x = "Data"),
	ggplot(df) +
		geom_histogram(aes(x = Spread1), alpha = 0.7, fill = "#72afff", bins = 100) +
		geom_histogram(aes(x = Spread2), alpha = 0.7, fill = "#4f9bff", bins = 100) +
		geom_text(aes(x = 2.5, y = 200, label = "σ = 2")) +
		geom_text(aes(x = 1, y = 600, label = "σ = 1")) +
		labs(x = "Data", y = "Frequency", title = "Differing Standard Deviations"),
	ggplot(df) +
		geom_histogram(aes(x = Kurt2), alpha = 0.5, fill = "red", bins = 100) +
		geom_histogram(aes(x = Kurt1), alpha = 0.5, fill = "orange", bins = 100) +
		geom_text(aes(x = 4, y = 50, label = "κ high")) +
		geom_text(aes(x = 0.75, y = 550, label = "κ low")) +
		xlim(c(-7, 7)) +
		labs(x = "Data", y = "Frequency", title = "Differing Kurtosis"),
	ggplot(df) +
		geom_point(aes(x = Norm1, y = Norm2), alpha = 0.2, color = "dark violet") +
		labs(x = "Data X", y = "Data Y", title = "Correlation"),
	ggplot(iris) +
		geom_point(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +
		labs(title = "Classes") + theme(legend.position = "none"),
	ggplot(faithful) +
		geom_point(aes(x = eruptions, y = waiting), color = "gold") +
		labs(title = "Clusters")
)

grid.arrange(grobs = plots, nrow = 2)

{% endhighlight %}

</details> <br>

<details>
<summary> b: R code for obtaining the KL divergence image. </summary>

{%highlight ruby%}

######## Beta Binomial Mass
dbetabin <- Vectorize(function(x, n, a, b) choose(n, x) * beta(x + a, n - x + b) / beta(a, b))

######## BetaBinomial a MME
a <- function(x, b) (mean(x) * b / (5 - mean(x)))

######## BetaBinomial b Squared Error for MME
b_diff <- function(b, x){
  t <- var(x) - ((5 * a(x, b) * b) * (a(x, b) + b + 5) / (((a(x, b)  + b)^2) * (a(x, b) + b + 1)))
  return(t^2)
}

######## KL Divergence between empirical mass and binomial MLE
Dklb <- function(x){
  x_1 <- x[1:floor(length(x)/2)]
  x_2 <- x[(floor(length(x)/2) + 1):length(x)]
  P <- numeric(5 + 1)
  P[as.numeric(names(table(x_1))) + 1] <- as.numeric(table(x_1))
  P <- P/length(x_1)
  Q <- dbinom(0:5, 5, mean(x_2)/5)
  log_vals <- log(P/Q)
  log_vals[is.infinite(log_vals)] <- 0
  return(sum(P * log_vals))
}

######## KL Divergence between empirical mass and betabinomial MME
Dklbb <- function(x){
  x_1 <- x[1:floor(length(x)/2)]
  x_2 <- x[(floor(length(x)/2) + 1):length(x)]
  P <- numeric(5 + 1)
  P[as.numeric(names(table(x_1))) + 1] <- as.numeric(table(x_1))
  P <- P/length(x_1)
  b <- min(suppressWarnings(optim(1, b_diff, x = x_2)$par), 300)
  a <- a(x_2, b)
  Q <- dbetabin(0:5, 5, a, b)
  log_vals <- log(P/Q)
  log_vals[is.infinite(log_vals)] <- 0
  return(sum(P * log_vals))
}

######## To prevent non-singular values
nr <- function(x){
  if(x == 0) return(0.001)
  if(x == 1) return(0.999)
  return(x)
}

KLbin <- numeric(10000)
KLbetabin <- numeric(10000)

for(i in 1:10000){
  s <- rbinom(10, 5, 0.5)
  KLbin[i] <- Dklb(s)
  KLbetabin[i] <- Dklbb(s)
}

LLbin <- numeric(10000)
LLbetabin <- numeric(10000)

for(i in 1:10000){
  s <- rbinom(10, 5, 0.5)
  s_1 <- s[1:5]; s_2 <- s[6:10]
  LLbin[i] <- sum(log(dbinom(s_2, 5, mean(s_1)/5)))
  b <- min(suppressWarnings(optim(1, b_diff, x = s_1)$par), 50)
  a_ <- a(s_1, b)
  LLbetabin[i] <- sum(log(dbetabin(s_2, 5, a_, b)))
}

library(ggplot2)

g1 <- ggplot(data.frame("Bin" = KLbin, "BetaBin" = KLbetabin)) +
  geom_point(aes(x = Bin, y = BetaBin), alpha = 0.6, color = "orange") +
  labs(title = "KL-Divergence(Empirical Mass | Best Fit)")

g2 <- ggplot(data.frame("Bin" = LLbin, "BetaBin" = LLbetabin)) +
  geom_point(aes(x = Bin, y = BetaBin), alpha = 0.6, color = "orange") +
  labs(title = "LL Comparison")

gridExtra::grid.arrange(g1, g2, nrow = 1)

{% endhighlight %}

</details> <br>

<details>
<summary> c: R Code for Regularized regression images. </summary>

{%highlight ruby%}

library(ggplot2)
library(ellipse)
library(analysis)

e <- as.data.frame(ellipse(0.95))

df <- data.frame(x = e$x, y_1 = e$y)
df$poly_x <- rep(c(-1, 0, 1, 0), 25)
df$poly_y <- rep(c(0, -1, 0, 1), 25)
df$line_x <- seq(0.025, 2.5, 0.025)
df$line_y <- sapply(2 * df$line_x - 2.5, function(x) max(x, 0))

ggplot(df) + xlim(c(-5, 5)) + ylim(c(-5, 5)) +
  geom_polygon(aes(x = 0.68*x + 2.5, y = 1.36*y_1 + 2.5), fill = "#aaaaaa") +
  geom_polygon(aes(x = 0.415*x + 2.5, y = 0.83*y_1 + 2.5), fill = "#606060") +
  geom_polygon(aes(x = 0.2*x + 2.5, y = 0.4*y_1 + 2.5), fill = "#111111") +
  geom_polygon(aes(x = poly_x, y = poly_y), fill = "#54b7ff", alpha = 0.6) +
  geom_polygon(aes(x = poly_x*2, y = poly_y*2), fill = "#75c5ff", alpha = 0.4) +
  geom_polygon(aes(x = poly_x*3, y = poly_y*3), fill = "#96d3ff", alpha = 0.2) +
  geom_line(aes(x = line_x, y = line_y), color = "orange", lwd = 1.25, alpha = cos(df$line_x * pi/5)) +
  geom_text(aes(x = 2, y = 4, label = "g(β)")) + geom_text(aes(x = -2, y = 2.5, label = "||h(β)|| < k")) + 
  xlab("β1") + ylab("β2") +
  theme(axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks.x=element_blank(), axis.ticks.y=element_blank())
 
df$circ_x <- rep(seq(-1, 1, length.out = 50), 2)
df$circ_y <- c(sqrt(1 - df$circ_x[1:50]^2), -sqrt(1 - df$circ_x[51:100]^2))
df$line_x <- seq(0.025, 2.5, 0.025)
c = -0.156; b = -0.714; a = 0.775;
df$line_y <- c(sapply(df$line_x[1:60], function(x) a*(x^2) + b*x + c),
			   sapply(2 * df$line_x[61:100] - 2.5, function(x) max(x, 0)))

ggplot(df) + xlim(c(-5, 5)) + ylim(c(-5, 5)) +
  geom_polygon(aes(x = 0.68*x + 2.5, y = 1.36*y_1 + 2.5), fill = "#aaaaaa") +
  geom_polygon(aes(x = 0.415*x + 2.5, y = 0.83*y_1 + 2.5), fill = "#606060") +
  geom_polygon(aes(x = 0.2*x + 2.5, y = 0.4*y_1 + 2.5), fill = "#111111") +
  geom_polygon(aes(x = circ_x, y = circ_y), fill = "#42ff2d", alpha = 0.6) +
  geom_polygon(aes(x = circ_x*2, y = circ_y*2), fill = "#77ff68", alpha = 0.4) +
  geom_polygon(aes(x = circ_x*3, y = circ_y*3), fill = "#a3ff99", alpha = 0.2) +
  geom_line(aes(x = line_x, y = line_y), color = "#ff47c7", lwd = 1.25, alpha = cos(df$line_x * pi/5)) +
  geom_text(aes(x = 2, y = 4, label = "g(β)")) + geom_text(aes(x = -2, y = 2.5, label = "||h(β)|| < k")) + 
  xlab("β1") + ylab("β2") +
  theme(axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

{% endhighlight %}

</details> <br>

<details>
<summary> d: R Code for Likelihood Sampling Distributions. </summary>

{%highlight ruby%}

l_samp <- dnorm(rnorm(10000))
lm_samp <- sapply(1:10000, function(i) prod(dnorm(rnorm(10)))^0.1)
l10_samp <- sapply(1:10000, function(i) prod(dnorm(rnorm(10))))
ll_samp <- log(dnorm(rnorm(10000)))
llm_samp <- sapply(1:10000, function(i) mean(log(dnorm(rnorm(10)))))
ll10_samp <- sapply(1:10000, function(i) sum(log(dnorm(rnorm(10)))))

hists <- list(qplot(l_samp, fill = I(fancy::ggcols(50)[7]), xlab = "1pt Likelihood", main = "Sampling Distributions"),
			  qplot(ll_samp, fill = I(fancy::ggcols(50)[14]), xlab = "1pt Loglikelihood"),
			  qplot(lm_samp, fill = I(fancy::ggcols(50)[21]), xlab = "10pt Likelihood Geom Mean"),
			  qplot(l10_samp, fill = I(fancy::ggcols(50)[28]), xlab = "10pt Likelihood"),
			  qplot(llm_samp, fill = I(fancy::ggcols(50)[35]), xlab = "10pt Loglikelihood Mean"),
			  qplot(ll10_samp, fill = I(fancy::ggcols(50)[43]), xlab = "10pt Loglikelihood"))

gridExtra::grid.arrange(grobs = hists, nrow = 2)

{% endhighlight %}

</details> <br>

<details>
<summary> My histogram wrapper around ggplot2. </summary>

{%highlight ruby%}

hist_fancy <- function(...){

	args <- list(...); j = 1

	for(i in 1:length(args))
		if(toString(names(args)[i]) %in% c("", "NA")) { names(args)[i] <- paste0("x", j); j <- j + 1 }

	lengths <- sapply(args, length)
	numerics <- sapply(args, is.numeric)

	vars <- args[(lengths > 1) & numerics]
	args <- args[!((lengths > 1) & numerics)]

	overlaps <- intersect(c("fill", "xlim", "ylim"), names(vars))
	if(length(overlaps) != 0) {args[overlaps] <- vars[overlaps]; vars[overlaps] <- NULL}

	if(length(vars) == 0) stop("No variables found. Make sure to pass numerics of length two or above.")

	if("fill" %in% names(args)){
		factors <- unname(as.vector(unlist(sapply(names(vars),
			function(i) rep(args$fill[which(names(vars) == i)], each = lengths[i])))))
		args$fill <- NULL; factors <- I(factors)
	}else{
		factors <- unname(as.vector(unlist(sapply(names(vars),
			function(i) rep(i, each = lengths[i])))))
	}

	vars <- unname(unlist(vars))

	histogram <- function(...) ggplot2::qplot(vars, fill = factors, geom = "histogram", pos = "identity", ...)

	return(do.call("histogram", args))

}

{% endhighlight %}

</details> <br>

<details>
<summary> x: My GP Posterior Code </summary>

{%highlight ruby%}

GP_analyse_clear <- function(X1, Y1, X2, alpha, rho, sig_sq = 0.00001){
	
	X <- c(X1, X2)

	kern <- function(x1, x2, alpha, rho){
		(alpha^2) * exp(-1*((x1 - x2)^2)/(2*(rho^2)))
	}

	Sigma11 <- matrix(0, nrow = length(X1), ncol = length(X1))
	Sigma22 <- matrix(0, nrow = length(X2), ncol = length(X2))
	Sigma12 <- matrix(0, nrow = length(X1), ncol = length(X2))
	Sigma21 <- matrix(0, nrow = length(X2), ncol = length(X1))
	
	Sigma <- matrix(0, nrow = length(X), ncol = length(X))

	for(i in 1:length(X1)){
		for(j in 1:length(X1)){
			Sigma11[i, j] <- kern(X1[i], X1[j], alpha, rho)
			Sigma11[i, j] <- Sigma11[i, j] + ifelse(i == j, sig_sq, 0)
		}
	}

	for(i in 1:length(X2)){
		for(j in 1:length(X2)){
			Sigma22[i, j] <- kern(X2[i], X2[j], alpha, rho)
			Sigma22[i, j] <- Sigma22[i, j] + ifelse(i == j, sig_sq, 0)
		}
	}

	for(i in 1:length(X1)){
		for(j in 1:length(X2)){
			Sigma12[i, j] <- kern(X1[i], X2[j], alpha, rho)
		}
	}

	for(i in 1:length(X2)){
		for(j in 1:length(X1)){
			Sigma21[i, j] <- kern(X2[i], X1[j], alpha, rho)
		}
	}

	for(i in 1:length(X)){
		for(j in 1:length(X)){
			Sigma[i, j] <- kern(X[i], X[j], alpha, rho)
			Sigma[i, j] <- Sigma[i, j] + ifelse(i == j, sig_sq, 0)
		}
	}

	if(!identical(Sigma, cbind(rbind(Sigma11, Sigma21), rbind(Sigma12, Sigma22))))
		stop("Something is wrong with the covariance calculations")

	if(!identical(Sigma, t(Sigma)) | !Matrix::isSymmetric(Sigma))
		stop("Sigma doesn't equal its transpose")

	if(matrixcalc::is.positive.definite(Sigma)){
		warning("Sigma is not positive definite, using nearest +Def matrix")
		Sigma <- as.matrix(Matrix::nearPD(Sigma)$mat)
	}

	Mu_bar <- Sigma21 %*% solve(Sigma11) %*% Y1 # E(X2 | X1 = x1)
	Sg_bar <- Sigma22 - (Sigma21 %*% solve(Sigma11) %*% Sigma12) # V(X2 | X1 = x1)

	if(max(abs(Sigma / Sigma[1, 1])) > 1.05){
		stop("The Cauchy-Schwarz inequality has been broken")
	}

	if(!matrixcalc::is.symmetric.matrix(Sg_bar)){
		message("The conditional covariance matrix is not symmetric, rounding values to 6 d.p.")
		Sg_bar <- round(Sg_bar, 6)
	}

	if(!matrixcalc::is.positive.definite(Sg_bar)){
		message("The conditional covariance matrix is not positive definite, using nearest +Def matrix")
		Sg_bar <- as.matrix(Matrix::nearPD(Sg_bar)$mat)
	}

	return(list(Mu_Bar = Mu_bar, Sg_Bar = Sg_bar))

}

GP_sample <- function(Mu, Sg) MASS::mvrnorm(1, Mu, Sg)

{% endhighlight %}

</details> <br>

<details>
<summary> z: My ICA Code </summary>

{%highlight ruby%}

library(tuneR)
library(fastICA)

mix1 <- readWave("./mixedX.wav")
mix2 <- readWave("./mixedY.wav")

wave1 <- mix1@left
wave2 <- mix2@left

ica <- fastICA(data.frame(x = wave1, y = wave2), 2, method = "R", maxit = 250, tol = 1e-50, verbose = TRUE)

writeWave(Wave(ica$S[,1], samp.rate = 32000, bit = 16, pcm = TRUE), "./seperatedX.wav")
writeWave(Wave(ica$S[,2], samp.rate = 32000, bit = 16, pcm = TRUE), "./seperatedY.wav")

# BTW: You can use prcomp to do PCA :)

{% endhighlight %}

</details> <br>

<details>
<summary> f: R Code for Dimensionality Image </summary>

{%highlight ruby%}

library(mvtnorm); library(ggplot2)

dim_dist <- function(d, samp_size = 10, ...){
	samp <- rmvnorm(samp_size, mean = rep(0, d), sigma = diag(1, d, d))
	dists <- as.matrix(dist(samp, diag = T, upper = T, ...))
	return(mean(sapply(1:samp_size, function(i) min(dists[-i, i])/max(dists[-i, i]))))
}

qplot(x = 1:250,
	  y = sapply(1:250, function(d) dim_dist(d)),
	  main = "Euclidean Distance in High Dimensions",
	  ylab = "Average Ratio of Nearest to Furthest Point",
	  xlab = "Dimension", color = I("dark gray"), ylim = c(0, 1)) +
	  geom_hline(aes(yintercept = 1))

{% endhighlight %}

</details> <br>

## Stuff to add later

1. Rule of three: Probability theory gives us an awesome way to reason about the probability of failure of an event eve if we see no failures! Assume that a process represented by \\(X \sim Bin(n, p)\\) produces a set of \\(n\\) successes. We can still construct a posterior / confidence interval for p. This would obviously include 0 (and the highest mass would be there) but the interval at the 95% level (whatever that means) is close to \\([0, 3/n]\\).

2. MCMC aims to converge onto a stationary distribution which is at the posterior.

3. Some models have to deal with unidentifiability - multiple sets of parameters producing the same distribution. Bimodal / uniform terrain posteriors may be a sign of this. Maximum likelihood methods cannot handle such events. Moreover, ML techniques should be viewed with caution if the estimate happens at the parameter bounds (as this may be suggesting that the current model fails to account for some aspect of the data as the parameter is being forced to take values it otherwise cannot, just to be able to explain the data).

4. Differencing in ARIMA models can be viewed as a form of discrete derivative (a _difference_) - zero differences imply some form of stationarity, a single difference would correspond to a constant trend, a second order difference corresponds to a simple time varying trend, etc.

5. Some sources say that the likelihood was constructed _for_ Bayesian statistics although Fisher's ideas were extremely similar.

6. Gelman: purpose of model checking is to understand in what ways the model doesn't fit data. This is a nice comment - indeed, models that you build will not usually be "nice" on noisy real world data so a lot needs to be understood in every scenario to be able to do analysis correctly. He has also shared an opinion that the way marginal likelihoods are used now is not very meaningful.

7. Weakly informative priors are better than non-informative ones as they constrain the space enough so that an algorithm searching for the posterior will find it quickly. There seem to be other reasons as well.

8. The bootstrapped sample is a very good estimator of a transformation of a random sample. As I understand it, it's because the eCDF is a much better approximator of the "true" distribution, more so than approximations such as the Central Limit Theorem. This can be proved using Hermite polynomials.

9. When dealing with censored data, we can re-frame the problem as follows:

	Let \\(\tau\\) have the density \\(f_{\tau}\\) and distribution function \\(F_{\tau}\\).

	Now, let:

	$$ X \equiv \begin{cases} \tau,  & \text{if $\tau \leq T$} \\
	T, & \text{if $\tau \geq T$} \end{cases}$$

	Then, X is simply a compound random variable with a Bernoulli distribution.

	$$ \Leftrightarrow P_X  \begin{cases} \tau_{|\tau \leq T} \; \text{= $F(T)$} \\
	T \; \text{= $1 - F(T)$} \end{cases}$$

	Note that
	$$\;\; \tau_{|\tau \leq T} \;\;has\;\;density\;\; f_{\tau}/F(T)$$.

	Hence, the full likelihood of the observation model is:

	$$L = \prod_i^n f_{\tau}^{n_{\leq}} F^{-{n_{\leq}}} F^{n_{\leq}} (1 -F)^{n_{\geq}} = \prod_i^n f_{\tau}^{n_{\leq}} (1 -F)^{n_{\geq}} $$

10. Jacobians are ***extremely*** important in the context of probabilistic transformations.

	$$ if \;\; {\bf g}: \underline Y \rightarrow \underline X \;\; st \;\; {\bf g}^{-1}: \underline X \rightarrow \underline Y \;\; then \;\; f_{\underline Y} = f_{\underline X} ({\bf g}^{-1}) \left| \frac{\partial x_i}{\partial y_j} \right| $$

11. There are tests more powerful than the KS test but they may not be unbiased.

12. \\(ln \Gamma \\) has a left skew whereas \\(ln lgN \\) is symmetric

13. GPs, PGMs & Component Analyses are awesome.

14. Models to include: distributional, zero-inflated, Markovian (hidden, semi), stochastic, SDEs. Some of these are fairly unintuitive when you opt for more complex models.

15. As you use splines, GPs, polynomial interpolators, polynomial regressors, moving averages and filters for trend estimation in time series, you can use Fourier smoothing (i.e. pick high power frequencies on a spectrogram and fit a Fourier series to seasonality).

16. Exchangability: joint distribution remains the same for every permutation of indices. This can be nice to have if independence is too strong of a constraint.

17. Model parameters can be cross correlated when obtaining samples from an MCMC - this is to be expected.

18. Inference about the binomial number is ridiculously hard and the MLE is usually useless. This is why the binomial parameter n isn't _really_ a parameter.

19. Change of measure: the idea is to model the expected probabilistic behavior of a system based on the behavior of another.

20. Polynomial interpolation (i.e. Lagrange polynomials) can have messy oscillation in higher degrees. This is Runge's phenomenon and is a similar occurrence to the Gibbs phenomenon in Fourier series on steps.

21. Can do higher order interpolation using the bi-cubic / tri-cubic methods.

22. Independence of cumulants is characteristic of the Normal, but in general, cumulants are not independent (although the dependence can be quite weak - from my experiments).

23. VC dimension: number of points shattered by an algorithm (for neural networks it's very high, so you'd need many points not to overfit at all - but this result is controversial).

24. USE SIMULATIONS when probing functions of random variables.

24. Expressing some GP-type stochastic processes in a way can make them all tend to the GP using the central-limit theorem no matter what distribution (by me).

25. Probabilistic collocation methods / non intrusive spectral projections: these are non intrusive because you don't need to mess around with the internals of those functions. Split up the objective function in terms of a bunch of polynomials that are orthogonal in the expectation sense w. r. t. the underlying probability density and obtain moments easily.

26. Curse of dimensionality: Volumes of n-balls (_decreases_)[https://en.wikipedia.org/wiki/Volume_of_an_n-ball] after dimension 5 so the n-cube / n-ball volume ratio goes to infinity. Moreover, in high dimensions, most of the volume of an n-cube is (concentrated near its surface)[https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions] - e.g.: think about a cube of length one across every dimension in a high dimensional space; this has volume one. Now think of a cube with length 0.99 across every dimension in a 100-dimensional space. It's volume is just ***36.6%*** of the larger cube!

	As suggested in a paper on the above StackExchange source, we can compare the ratio of the average distance of the nearest point to the furthest point in a randomly sampled high dimensional normal. We see that the ratio approaches one:

	<img src = "/images/intro_f.png">

## Bibliography

1. _**Bayesian Data Analysis**_
  * Gelman A., Carlin J. B., Stern H. S., Dunson D. B., Vehtari A. & Rubin D. B., 2013.

2. _**Probability Theory: The Logic of Science**_
  * Jaynes E. T., 2003.

3. _**A Quick Review of Measure and Integration Theory**_
  * Tao T., 2009. _Accessible at [Terence Tao's Blog](https://terrytao.wordpress.com/2009/01/01/245b-notes-0-a-quick-review-of-measure-and-integration-theory/)._

4. _**Interpretations of Probability**_
  * Hájek, A., The Stanford Encyclopedia of Philosophy (Winter 2012 Edition), Zalta E. N. (ed.). Accessible at the [SEP](https://plato.stanford.edu/archives/win2012/entries/probability-interpret/).

5. _**Foundations of the Theory of Probability** (translated)_
  * Kolmogorov A. N., 1933. _Accessible at [york.ac.uk](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf)._

6. _**A Review of Probability Theory**_
  * Tao T. 2010. _Accessible at [Terence Tao's Blog](https://terrytao.wordpress.com/2010/01/01/254a-notes-0-a-review-of-probability-theory/)
