---
layout: posts
title:  "Random Stuff"
date:   2018-12-15 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Random Stuff

 1. Many models, particularly those that generate multi-modal posteriors are _unidentifiable_.

 2. Differencing in ARIMA models corresponds to the same logic as a derivative.

 3. The marginal likelihood can be a pretty bad model comparison tool.

 4. Weakly informative priors are better than non-informative ones as they constrain the space enough so that an algorithm searching for the posterior will find it quickly. Strong priors can add an incredible amount of information to a well specified model.

 5. The bootstrap constructs samples from an ECDF - transformations of these samples are very good estimators on practice. The fact that these are better than normal approximations can be proved using Hermite expansions.

 6. The Jacobian are ***extremely*** important in the context of probabilistic transformations.

	$$ if \;\; {\bf g}: \underline Y \rightarrow \underline X \;\; st \;\; {\bf g}^{-1}: \underline X \rightarrow \underline Y \;\; then \;\; f_{\underline Y} = f_{\underline X} ({\bf g}^{-1}) \left| \frac{\partial x_i}{\partial y_j} \right| $$

 7. \\(ln \Gamma \\) has a left skew whereas \\(ln lgN \\) is symmetric

 8. Component Analyses are awesome.

 9. Multivariate normal factor graphs are incredibly factorized; i.e. the precision matrix can be sparse, particularly in GPs with a monotonically decreasing covariance w.r.t. radius, but I've not been able to prove this theoretically.

 10. One way to prove the distribution of SDEs is to solve the Fokker-Planck equation.

 11. A covariance function can be derived using the spectral density and the Fourier transform in some fashion.

 12. Fourier smoothing can be used to approximate seasonality (i.e. pick high powered frequencies on a spectrogram and fit the corresponding Fourier series to the seasonality series).

 13. Inference about the number of experiments of a binomial distribution is ridiculously hard and the MLE is usually useless. This is why I maintain that the binomial parameter n isn't _really_ a parameter, as the number of data points is rarely ever treated as one.

 14. Change of measure: the idea is to model the expected probabilistic behavior of a system based on the behavior of another.

 15. Polynomial interpolation (i.e. Lagrange polynomials) can have messy oscillation in higher degrees. This is Runge's phenomenon and is a similar occurrence to the Gibbs phenomenon in Fourier series on steps.

 16. Independence of cumulants is characteristic of the Normal, and in general, cumulants are not independent.

 17. When probing non-monotonic transformations of random variables, one can use either Monte Carlo simulation or methods in uncertainty quantification (which break up the function into orthogonal components and calculate moments of an expected target distribution).
 
 18. Non-Gaussian GPs can ofter tend to normality due to the CLT.

 19. _**Maxent**_ modeling - if just particular facts are known about a scenario, a joint distribution can be found with maximum entropy that satisfies the given constraints. The exponential family is an example of a family of distributions with differing constraints. Moreover, the exponential family is characterized by sufficient statistics with non-increasing lengths w.r.t. the data, hence the "_**really**_ random"-ness.

 20. Treating GPs as latent variables acting on an observable is _amazing_. One can even solve differential equations using such an approach, particularly when working with good samplers or optimizers, e.g. HMC with Stan and Adam in Tensorflow.
 
 21. Stan code for simple radial basis function kernel interpolation of the form:
 
 $$f(x) = \sum_i \alpha_i e^{-(x - x_i)^2} $$
 
 {%highlight C%}
 real s(vector a, vector k, real x){
	return(sum(exp(-square(k - x)) .* a));
 } {% endhighlight %}
 
 It's really fast; it's O(n) and evaluates in 8.5ns/knot-length on my laptop.

 22. A while ago, I was experimenting with principal and independent component analyses. The ICA is amazing, I do not have the original garbled files and I don't remember where I got them from, but I managed to "unmix" the streams easily:

 <details>
 <summary> ICA Code </summary>
 
 {%highlight ruby%}
 
 library(tuneR)
 library(fastICA)
 
 mix1 <- readWave("./mixedX.wav")
 mix2 <- readWave("./mixedY.wav")
 
 wave1 <- mix1@left
 wave2 <- mix2@left
 
 ica <- fastICA(data.frame(x = wave1, y = wave2), 2, method = "R", maxit = 250, tol = 1e-50, verbose = TRUE)
 
 writeWave(Wave(ica$S[,1], samp.rate = 32000, bit = 16, pcm = TRUE), "./seperatedX.wav")
 writeWave(Wave(ica$S[,2], samp.rate = 32000, bit = 16, pcm = TRUE), "./seperatedY.wav")
 
 {% endhighlight %}
 
 </details>

 Separated Sample:
 <audio controls>
   <source src="/audio/sep.wav" type="audio/wav">
   Browser cannot play audio.
 </audio>
