---
layout: posts
title:  "SML with Spotify Data"
date:   2020-05-18 00:01:00 +0100
categories: stats python R
entries_layout: grid
---

## Data Science using Spotify

I thought it'd be fun to do an analysis of some public data that's outside my immediate domain. Here's the diary of this process.

## Data Management & Platform

The data that I'm using is the [Spotify](https://arxiv.org/abs/1901.09851) [Streaming Sessions Dataset](https://research.spotify.com/datasets).

Before I got into the task and modeling, I needed to figure out how to manage the data.

I'm running an Ubuntu Server (18.04) on a PC that I built two years ago. I've got 12GB of RAM, a GTX 1070 GPU, a 12-core i7 processor and around 80GB of space that I can dedicate to this project split across two SSDs. At the moment, the file systems I'm using are ext4 and exfat (I've heard about using OpenZFS for postgres and the Hadoop HDFS but I'm not going to try these out here because I don't know much about them, although I'm quite interested in both of these). If this was for production, I'd think a lot more about data redundancy, etc.

The data is around 50GB and sits as a collection of CSV files in a tar.gz. Initially, I considered using a postgres database and I copied over two CSVs (using R's `DBI::dbWriteTable`), but the table size (as seen in `/var/lib/postgresql/12/main/base/`) was larger than the CSV, so I'd go way over budget in terms of storage if I were to use postgres as is.

Instead, I opted for Spark. Spark/Sparklyr (an R API for Spark) supports lazy-reading of compressed CSVs and commands are optimized even when working on a single node. If I use tensorflow/keras for this project, I'll be able to write a generator using pandas for batch processing, so this setup should be sufficient for exploration and modeling (Spark also supports GLMs and some other stats/ML models).

Memory issues shouldn't be a problem as long as queries are written in a smart way. I also had to get the right version of the Java SDK working before this worked out.

<details>
<summary> (Click to Expand) R Code for Data Check </summary>
 
{%highlight R%}

library(sparklyr)
library(dplyr)

config = spark_config()
config$`sparklyr.shell.driver-memory` <- '15G'
config$spark.executor.memory <- '15G'
config$spark.memory.fraction <- 0.9

data_path = '/media/training_set/zipped/'
java_path = '/usr/lib/jvm/java-8-openjdk-amd64/'
spark_path = '/home/aditya/spark/spark-2.4.3-bin-hadoop2.7/'

Sys.setenv(JAVA_HOME = java_path)

sc = spark_connect(master = 'local', spark_home = spark_path)
data = spark_read_csv(sc, path = data_path, memory = F)

skips_by_song = data %>%
	group_by(track_id_clean, not_skipped) %>%
	summarize(count = n()) %>%
	collect()

{% endhighlight %}
 
</details> <br>

I'll also detail some bash commands that I normally use. Commands that I used in this section:
 * `ssh -Y aditya@local_ip`: SSH with X11 forwarding
 * `psql`: PostgreSQL Terminal
 * `sudo (-u)`: To establish dominance (as another user)
 * `sudo mount /dev/sda7 /media`: Mount external SSD to `/media`
 * `sudo fdisk -l`: Disk information
 * `ls -lha`: List all, in a human friendly way
 * `cd -`: Go back to previous directory
 * `apropos`: Help with command, list of similar commands
 * `sudo chmod -R 777 media/`: CAUTION advised, make all files read/write-able by everyone
 * `tar -xvf` and `tar -tvf file.tar.gz > file_list`: Extract file, list files in tar archive
 * `find ~ -maxdepth 5 -name x*`: Find file containing 'x'

### More Coming Soon
