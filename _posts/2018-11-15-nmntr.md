---
layout: posts
title:  "Non-Monotonic Transforms"
date:   2018-11-15 00:00:00 +0100
categories: stats R
entries_layout: grid
---

## Exact Densities for Non-Monotonic Transformations (draft)

This post was inspired by a late night epiphany that it should be possible to treat non-monotonic functions as piecewise monotonic and use the change of variables technique to obtain densities... and lo, you can:

> If two random variables \\(X\\) and \\(Y\\) are related as \\(Y = g(X)\\), where \\(g\\) is a differentiable non-monotonic (but piecewise monotonic) function, and if the density of \\(X\\) is known to be \\(p_X\\), then the density of \\(Y\\) can be expressed as:
> 
> $$p_Y(y) = \sum_{x_i \in g^{-1}(y)} p_X(x_i) / \left| \frac{dg}{dx}(x_i) \right|$$
> 
> ... where the sum is over all values \\(x_i\\) in the preimage set corresponding to \\(y\\).

References:
 1. [thirdorderscient's blog](http://thirdorderscientist.org/homoclinic-orbit/2013/5/13/non-monotonic-transformations-of-random-variables) / [Kobayashi, Mark and Turin's Probability, Random Processes, and Statistical Analysis](https://www.cambridge.org/core/books/probability-random-processes-and-statistical-analysis/1909C657E4758038B54C4235B3AD0FDF)
 2. [wikipedia](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables)

My first attempt before seeing this result was to run the Metropolis-Hastings by using just a single term in the density of \\(X\\), which was the density of \\(Y\\) multiplied its Jacobian at that point and surprisingly, this gives very good results anyway - saving the need to calculate the set of inverse values. Perhaps this is due to some kind of an averaging effect that the MCMC produces.

---

### <span style="color:violet"> Proof of the Exact Density </span>

From the KMT book, one (handwavy way) to prove the exact result is as follows:

<center> <img src="/images/nmprf.png"> </center>

For any given length \\(y + \delta y\\), there are multiple lengths in the preimage of the function\\(g\\), here, \\(x_1 + \delta x_1, x_2 + \delta x_2, x_3 + \delta x_3\\). Note that \\(\delta x_2\\) is negative.

$$ P(y \leq Y \leq y + \delta y) = P(x_1 \leq X \leq x_1 + \delta x_1) + P(x_2 + \delta x_2 \leq X \leq x_2) + P(x_1 \leq X \leq x_1 + \delta x_1) $$

$$\Rightarrow p_Y(y) \delta y \approx p_X(x_1) \delta x_1 - p_X(x_2) \delta x_2 + p_X(x_3) \delta x_3 $$

$$\Rightarrow p_Y(y) = p_X(x_1) \frac{dx}{dy} \Big |_{x_ 1} - p_X(x_2) \frac{dx}{dy} \Big |_{x_ 2} + p_X(x_3) \frac{dx}{dy} \Big |_{x_ 3} $$

Noting that \\(x = g^{-1}(y)\\) and that the second derivative term in the equation above is negative, the solution follows.

<details>
<summary> R Code for the Graph </summary>

{%highlight ruby%}

### SETUP

x <- seq(-5, 5, length.out = 200)

f <- function(x) x^3 - 5*x + 2*cos(pi * x)
y <- Vectorize(f)(x) # Vectorize in case if broadcasting fails for more complex functions

### PLOT

plot(x, y, xaxt = "n", yaxt = "n", type = "l")
abline(v = c(-2.34, 0.26, 2.01), h = 0, lty = 3)
text(-2, -13.5, "x1 + δx1")
text(0.5, -13.5, "x2 + δx2")
text(2.1, -13.5, "x3 + δx3")
text(-2.6, 0.3, "y + δy")

{% endhighlight %}

</details>

---

The great thing deal about this method is that one has the density outright - this can then be fed into other MCMC samplers in combination with more complex models (which wouldn't be the case with plain Monte Carlo).

The preimage set can be a pain to obtain, but one possible way is to get it is by fitting a spline to the function \\(g\\), breaking it up into monotonic pieces and inverting them. Both the smoothing spline and GPs are suitable here as they are differentiable and derivative values can be obtained explicitly.

---

### <span style="color:green"> Simulation </span>

The function in this demonstration is:

$$Y = X^3 - 5X + 2\cos(\pi X)$$

<center> <img src="/images/nmdmo.png"> </center>

Obtaining the density of \\(Y\\) and using MCMC to sample from it gives us a sample from \\(Y\\) but we could've used Monte Carlo simulation to obtain samples from the distribution as well. Comparing the two methods results in this QQ plot:

<center> <img src="/images/nmtqq.png"> </center>

<details>
<summary> MCMC Code </summary>

{%highlight ruby%}

### The code is messy at the moment, I will clean it up and vectorize later

library(ggplot2)

qplot(x, y, geom = "line", main = "Example Function")

mcmc <- function(target){

    X <- numeric(10000)

    X[1] <- runif(1)

    for(i in 2:length(X)){
        proposal <- rnorm(1, X[i - 1], 2)
        a <- target(proposal)/(target(X[i - 1]) + 0.00001)
        X[i] <- ifelse(runif(1) <= a, proposal, X[i - 1])
    }

    return(X)
    
}

fun_box <- function(){
    
    model <- smooth.spline(x, y) # fit the spline to the full data set
    derv <- as.data.frame(predict(model, x, deriv = 1)) # get the derivatives to identify monotonic regions
    
    ends <- cumsum(rle(derv$y >= 0)$lengths) # derv > 0 identifies monotonic runs and rle measures their length
    starts <- c(1, 1 + ends[-length(ends)]) # starts measures where runs start and ends records where they end
  
    n_segments <- length(ends) # number of monotonic segments in the line
    
    mods_list <- list()
    
    # fit a spline for each segment; y and x are reversed here as we're fitting a spline to the inverse function 
    for(i in 1:n_segments){
        s <- starts[i] # start point
        e <- ends[i] # start point
        mods_list[paste0("m", i)] <- list(spline(y[s:e], x[s:e]))
    }
    
    inv_func <- function(y_prop){

        rec_x <- numeric(n_segments)
        rec_d <- numeric(n_segments)

        # for each segment, predict the inverse and accept it only if the prediction lies within
        # the domain of the fit segment. E.g. t=if a segment was fit where x was in [0, 1], the
        # inverse shouldn't extrapolate outside [0, 1].
        for(i in 1:n_segments){
            rec_x[i] <- approxfun(mods_list[[i]])(y_prop)
            if(!is.na(rec_x[i])){
                rec_d[i] <- predict(model, rec_x[i], deriv = 1)$y
            }else{
                rec_x[i] <- 0
                rec_d[i] <- Inf
            }
        }

        return(list(x = rec_x, d = rec_d))
    }

    # finally calculate the density as in the equation above
    target <- function(y_prop){
        inverse_info <- inv_func(y_prop)
        return(sum(dnorm(inverse_info$x) / abs(inverse_info$d)))
    }

    return(mcmc(target))    
    
}

results <- fun_box()

qplot(results)

{% endhighlight %}

</details>

... it works!

## The More Interesting Problem

We can use this methodology to answer a really interesting question: _**Is it possible to sample from \\(X\\) if the distribution of \\(Y\\) is known instead?**_ (I think so...).

We can use this method to set informative priors in situations where information is available about a complex transformation of the prior and not the prior itself. Perhaps this method can be used to obtain solutions for Bayesian inverse problems as well.

---

_If you see any errors in the post or if you'd like to comment, please [create an issue on GitHub](https://github.com/InfProbSciX/infprobscix.github.io/issues)._
