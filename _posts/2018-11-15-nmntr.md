---
layout: posts
title:  "Non-Monotonic Transforms"
date:   2018-11-15 00:00:00 +0100
categories: stats R
entries_layout: grid
---

## Exact Densities for Non-Monotonic Transformations (draft)

This post was inspired by a late night epiphany that it should be possible to treat non-monotonic functions as piecewise monotonic and use the change of variables technique to obtain densities... and lo, you can:

> If two random variables \\(X\\) and \\(Y\\) are related as \\(Y = g(X)\\), where \\(g\\) is a differentiable non-monotonic (but piecewise monotonic) function, and if the density of \\(X\\) is known to be \\(p_X\\), then the density of \\(Y\\) can be expressed as:
> 
> $$p_Y(y) = \sum_{x_i \in g^{-1}(y)} p_X(x_i) \left| \frac{dg^{-1}_i(y)}{dy} \right|_.$$
> 
> ... where the sum is over all values \\(x_i\\) in the preimage set corresponding to \\(y\\).

References:
 1. [thirdorderscient's blog](http://thirdorderscientist.org/homoclinic-orbit/2013/5/13/non-monotonic-transformations-of-random-variables) / [Kobayashi, Mark and Turin's Probability, Random Processes, and Statistical Analysis](https://www.cambridge.org/core/books/probability-random-processes-and-statistical-analysis/1909C657E4758038B54C4235B3AD0FDF)
 2. [wikipedia](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables)

My first attempt before seeing this result was to run the Metropolis-Hastings by using just a single term in the density of \\(X\\), which was the density of \\(Y\\) multiplied its Jacobian at that point and surprisingly, this gives very good results anyway - saving the need to calculate the set of inverse values. Perhaps this is due to some kind of an averaging effect that the MCMC produces.

---

### <span style="color:violet"> Proof </span>

From the KMT book, one (handwavy way) to prove the exact result is as follows:

<center> <img src="/images/nmprf.png"> </center>

For any given length \\(y + \delta y\\), there are multiple lengths in the preimage of the function\\(g\\), here, \\(x_1 + \delta x_1, x_2 + \delta x_2, x_3 + \delta x_3\\). Note that \\(\delta x_2\\) is negative.

$$ P(y \leq Y \leq y + \delta y) = P(x_1 \leq X \leq x_1 + \delta x_1) + P(x_2 + \delta x_2 \leq X \leq x_2) + P(x_1 \leq X \leq x_1 + \delta x_1) $$

$$\Rightarrow p_Y(y) \delta y \approx p_X(x_1) \delta x_1 - p_X(x_2) \delta x_2 + p_X(x_3) \delta x_3 $$

$$\Rightarrow p_Y(y) = p_X(x_1) \frac{dx}{dy} \Big |_{x_ 1} - p_X(x_2) \frac{dx}{dy} \Big |_{x_ 2} + p_X(x_3) \frac{dx}{dy} \Big |_{x_ 3} $$

Noting that \\(x = g^{-1}(y)\\) and that the second derivative term in the equation above is negative, the solution follows.

<details>
<summary> R Code for the Graph </summary>

{%highlight ruby%}

### SETUP

x <- seq(-3, 3, length.out = 200)

f <- function(x) x^3 - 5*x + 2*cos(pi * x)
y <- Vectorize(f)(x) # Vectorize in case if broadcasting fails for more complex functions

### PLOT

plot(x, y, xaxt = "n", yaxt = "n", type = "l")
abline(v = c(-2.34, 0.26, 2.01), h = 0, lty = 3)
text(-2, -13.5, "x1 + δx1")
text(0.5, -13.5, "x2 + δx2")
text(2.1, -13.5, "x3 + δx3")
text(-2.6, 0.3, "y + δy")

{% endhighlight %}

</details>

---

The advantage of this method One could use Monte-Carlo Simulations as well, to directly 

The preimage set can be a pain to obtain. In the following demonstration, to get the derivatives of the inverse for any black box function, I first fit a spline to the function \\(g\\), break it up into monotonic bits and invert the function. Both the smoothing spline and GPs are differentiable and derivative values can be obtained explicitly.

The demonstration will be done on the same non-monotonic function as above:

$$Y = X^3 - 5X + 2\cos(\pi X)$$

<center> <img src="/images/nmdmo.png"> </center>

Obtaining the density and using MCMC to sample from Y gives us a sample from \\(Y\\) but we could've used Monte Carlo simulations to obtain the density as well. Comparing the two methods results in this QQ plot:

<center> <img src="/images/nmtqq.png"> </center>

<details>
<summary> MCMC Code </summary>

{%highlight ruby%}

### The code is messy at the moment, I will clean it up and vectorize later

library(ggplot2)

qplot(x, y, geom = "line", main = "Example Function")

mcmc <- function(target){

    X <- numeric(100000)

    X[1] <- runif(1)

    for(i in 2:length(X)){
        proposal <- rnorm(1, X[i - 1], 0.5)
        a <- target(proposal)/(target(X[i - 1]) + 0.00001)
        X[i] <- ifelse(runif(1) <= a, proposal, X[i - 1])
    }

    return(X)
    
}

fun_box <- function(){
    
    model <- smooth.spline(x, y) # fit the spline to the full data set
    derv <- as.data.frame(predict(model, x, deriv = 1)) # get the derivatives to identify monotonic regions
    
    ends <- cumsum(rle(derv$y >= 0)$lengths) # derv > 0 identifies monotonic runs and rle measures their length
    starts <- c(1, 1 + ends[-length(ends)]) # starts measures where runs start and ends records where they end
  
    n_segments <- length(ends) # number of monotonic segments in the line
    
    mods_list <- list()
    
    # fit a spline for each segment; y and x are reversed here as we're fitting a spline to the inverse function 
    for(i in 1:n_segments){
        s <- starts[i] # start point
        e <- ends[i] # start point
        mods_list[paste0("m", i)] <- list(smooth.spline(y[s:e], x[s:e]))
    }
    
    inv_func <- function(y_prop){

        rec_x <- numeric(n_segments)
        rec_d <- numeric(n_segments)

        # for each segment, predict the inverse and accept it only if the prediction lies within
        # the domain of the fit segment. E.g. t=if a segment was fit where x was in [0, 1], the
        # inverse shouldn't extrapolate outside [0, 1].
        for(i in 1:n_segments){
            rec_x[i] <- predict(mods_list[[i]], y_prop)$y
            rec_d[i] <- predict(mods_list[[i]], y_prop, deriv = 1)$y
            rec_d[i] <- ifelse(rec_x[i] >= x[starts[i]] & rec_x[i] <= x[ends[i]], rec_d[i], 0)
            rec_x[i] <- ifelse(rec_x[i] >= x[starts[i]] & rec_x[i] <= x[ends[i]], rec_x[i], 0)
        }

        return(list(x = rec_x, d = rec_d))
    }

    # finally calculate the density as in the equation above
    target <- function(y_prop){
        inverse_info <- inv_func(y_prop)
        return(sum(dnorm(inverse_info$x) * inverse_info$d))
    }

    return(mcmc(target))    
    
}

results <- fun_box()

qplot(results)

{% endhighlight %}

</details>

The results surely aren't that great, but I suspect that this is due to the derivative approximation that I'm using.

## Another Interesting Problem

We can use this methodology to answer a really interesting question: _**Is it possible to sample from \\(X\\) if the distribution of \\(Y\\) is known instead?**_.

A simple simulation by modifying the code above results in a definitive yes; here, we set the distribution of \\(Y\\) to be standard normal and construct a sample \\(X\\) by using the method above - simply replace Y by X and notice that \\( {g^{-1}}' = {g'}^{-1} \\).

---

_If you see any errors in the post or if you'd like to comment, please [create an issue on GitHub](https://github.com/InfProbSciX/infprobscix.github.io/issues)._
