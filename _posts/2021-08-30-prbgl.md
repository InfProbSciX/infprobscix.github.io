---
layout: posts
title:  "A Probabilistic Interpretation of the Griffin-Lim Algorithm"
date:   2021-08-31 00:01:00 +0000
categories: python
entries_layout: grid
---

This is a draft post. Here is an attempt to interpret the Griffin-Lim loss function as a likelihood.

The Griffin-Lim algorithm, given the absolute value of an STFT $$A$$, tries to find a complex matrix $$X = A \exp(\theta i)$$ such that the frobenius norm:

$$|| \mathcal{G} \mathcal{G}^{\dagger}X - X ||^2$$

... is minimised [Deep Griffinâ€“Lim Iteration, 2019, Masuyama et al.]. Here, $$\mathcal{G}$$ corresponds to the STFT operation, and $$\mathcal{G}^{\dagger}$$ to the inverse. Generally, this normal with random $$\theta$$ will result in a non-zero norm, due to the fact that there's redundant information (one point contributes frequencies to multiple windows) and not all phases correspond to sensible signals (the inverse fourier transform has to be real, first angle needs to be 0, the mirrored frequencies are the complex conjugate of the preceding frequencies, etc.).

The STFT operation is as follows - a signal is first segmented, and a fourier transform is computed for each segment e.g.:

$$ \{1, ..., 8\} \mapsto \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \\ 3 & 5 & 7 \\ 4 & 6 & 8 \end{bmatrix} \overset{D\times}{\mapsto} \frac{1}{2}\begin{bmatrix}
1 &  1 &  1 &  1\\
1 &  -i & -1 & i\\
1 & -1 &  1 & -1\\
1 & i & -1 & -i\end{bmatrix} \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \\ 3 & 5 & 7 \\ 4 & 6 & 8 \end{bmatrix} $$

... where $$D$$ is the [discrete fourier transform matrix](https://en.wikipedia.org/wiki/DFT_matrix). In the example above, the number of points hopped between windows is two and number of points used for the FT is four.

The inverse STFT operation first obtains the real part of the inverse fourier transform of the STFT matrix, then performs an overlap add - i.e. averages together (weighted by the window function) different windows' contribution towards a particular signal point.

$$ \begin{bmatrix} STFT \end{bmatrix} \overset{D^{\dagger}\times}{\mapsto} \begin{bmatrix} 1 & 3_2 & 5_2 \\ 2 & 4_2 & 6_2 \\ 3_1 & 5_1 & 7 \\ 4_1 & 6_1 & 8 \end{bmatrix} \mapsto \{1, 2, \tfrac{1}{2}3_1 + \tfrac{1}{2}3_2, ...\} $$

Observations have been subscript by when they first appear. $$D^{\dagger}$$ is the conjugate transpose of $$D$$. The DFT matrix is symmetric and unitary ($$D D^{\dagger} = \mathbb 1$$).

Let's set up an example problem in python:

{% highlight python %}

import numpy as np
from scipy.linalg import dft

np.random.seed(42)

n = 8; n_fft = 4; hop_length = 2
n_frames = n//hop_length - 1

D = dft(n_fft)

x = np.random.normal(size=n).reshape(-1, 1)

{% endhighlight %}

Using the [vec-trick](https://en.wikipedia.org/wiki/Kronecker_product#Matrix_equations):

$$\left(\mathbf{B}^\textsf{T} \otimes \mathbf{A}\right) \, \operatorname{vec}(\mathbf{X}) = \operatorname{vec}(\mathbf{AXB}) = \operatorname{vec}(\mathbf{C})$$

... I vectorise the STFT matrix and compute the STFT operation as a matrix operation:

$$\operatorname{vec}(STFT(s)) = \operatorname{vec}(DWs) = (\mathbb{1} \otimes D) s \equiv D_k W s, $$

where $$Ws$$ repeats the signal as necessary to split it up into segments, and $$D_k = (\mathbb{1} \otimes D)$$ is the kronecker product. W looks like:

$$\begin{bmatrix} 1\\&1\\&&1\\&&&1\\&&1\\&&&1\\&&&&1&\cdots\end{bmatrix}$$

Similarly, one can also define the inverse STFT as a matrix operation:

$$s = W_i \Re \left[ D_k^{\dagger} \begin{bmatrix}STFT\end{bmatrix} \right]$$

... where $$W_i$$ corresponds to the overlap-add operation.

This interpretation shows that the GLA loss corresponds to the likelihood (ignoring constants that don't depend on the STFT) of a zero-mean degenerate complex normal distribution.

The code below shows how these matrices can be produced, and how the STFT and the inverse STFT can be defined.

{% highlight python %}

Dk = np.kron(np.eye(n_frames), D/n_fft**0.5)
Dhk = Dk.conj().T

W = np.zeros((n, n_frames))
for i in range(n_frames):
    W[(i*hop_length):(i*hop_length + n_fft), i] = 1

W /= (W @ np.ones((n_frames, 1)))

W_i = np.zeros((n, n_frames*n_fft))
for i in range(n_frames):
    idx_min, idx_max = i*hop_length, i*hop_length + n_fft
    W_i[idx_min:idx_max, (i*n_fft):((i + 1)*n_fft)] = np.diag(W[idx_min:idx_max, i])

W = np.ceil(W_i.T)

def stft(x):
    return Dk @ W @ x

def istft(stft_matrix):
    return W_i @ (Dhk @ S).real

S = stft(x)
# np.abs(istft(S) - x).max() # 1.7e-16

{% endhighlight %}

Therefore, the Griffin-Lim loss function becomes (splitting the real part into half z plus half z conjugate):

$$\left| \left| D_kW W_i\Re\left[ D_k^{\dagger} X \right] - X \right|\right|^2 = \left| \left| \tfrac{1}{2}D_kW W_i D_k \bar{X} + (\tfrac{1}{2}D_kW W_i D_k^{\dagger}-\mathbb{1})X \right|\right|^2$$

$$\equiv \left| \left| A\bar{X} + BX \right|\right|^2 \equiv \left| \left| N \right|\right|^2 $$

The calculation of matrices $$A, B$$ is as follows:

{% highlight python %}

A = 0.5 * Dk @ W @ W_i @ Dk
B = (0.5 * Dk @ W @ W_i @ Dhk) - np.eye(len(S))

{% endhighlight %}

Then,

$$ \left| \left| N \right|\right|^2 = N^{\dagger}N= (X^TA^{\dagger} + X^{\dagger}B^{\dagger})(A\bar{X} + BX) $$

$$=X^TA^{\dagger}A\bar{X} + X^TA^{\dagger}BX + X^{\dagger}B^{\dagger}A\bar{X} + X^{\dagger}B^{\dagger}BX $$

and as $$tr(A)=tr(A^{T})$$,

$$ N^{\dagger}N = X^TA^{\dagger}BX + X^{\dagger}B^{\dagger}A\bar{X} + X^{\dagger}(A^T\bar{A} + B^{\dagger}B)X.$$

Comparing this with the [multivariate complex normal distribution's density](https://en.wikipedia.org/wiki/Complex_normal_distribution#Density_function), we find:

$$-\overline{P^{-1}} = A^T\bar{A} + B^{\dagger}B, \,\,\, \mu = 0, \,\,\,  \tfrac{1}{2}R^T\overline{P^{-1}} = A^{\dagger}B,  \,\,\,  \tfrac{1}{2}R^{\dagger}P^{-1} = B^{\dagger}A $$

Rearranging, we can compute:

{% highlight python %}

P_inv = (A.T @ A.conj()) + (B.conj().T @ B)
P_inv = -P_inv.conj()
P = np.linalg.inv(P_inv)

R = (2 * B.conj().T @ A @ P).conj().T

# verify R is correct
(0.5*R.T @ P_inv.conj()) - (A.conj().T @ B) # it is - all zeros

R = R.round(10)
P = P.round(10)
P_inv = P_inv.round(10)

{% endhighlight %}

Therefore, we've written the GLA loss as the log density of a zero-mean complex normal distribution evaluated at $$X$$ with the given parameters (ignoring constant terms). Below, as an example, we modify the angles in the true STFT:

{% highlight python %}

S = np.abs(S)
theta = np.random.uniform(-np.pi, np.pi, size=(n_fft, n_frames))
theta[0, :] = 0
theta[3, :] = -theta[1, :]
S = S * np.exp(theta * 1j).reshape(-1, 1, order='F')

# calculate N using the two methods shown and diff:
# (A @ S.conj() + B @ S - stft(istft(S)) + S).round(10) # zeros

# calculate N^H N and see if this is the same as normal log density:
# (-S.conj().T @ P_inv.conj() @ S).real + (S.T @ R.T @ P_inv.conj() @ S).real - \
#     (np.abs(A @ S.conj() + B @ S)**2).sum()


{% endhighlight %}

The determinant terms in the complex normal log density (the terms that aren't dependent on the STFT) are zero however, and it seems to be the case that the covariance and pseudo-covariance are degenerate.

Todo:
 * verify implementations for other `n, n_fft, hop_length`s
 * try to find a non-degenerate log density and corresponding parameters
 * replace $$A, B$$ with the explicit set of matrices.

---

Some more tests to see if we're consistent with the librosa implementations.

{% highlight python %}

from librosa import stft as stft_l, istft as istft_l
S_cut = S.reshape(n_fft, n_frames, order='F')[:3, :]

(stft_l(istft_l(S_cut, window='boxcar', hop_length=hop_length, center=False), \
    n_fft=n_fft, window='boxcar', hop_length=hop_length, center=False) - S_cut).round(3) \
    - (stft(istft(S).reshape(-1, 1)) - S).reshape(n_fft, n_frames, order='F').round(3)[:3, :]

{% endhighlight %}
