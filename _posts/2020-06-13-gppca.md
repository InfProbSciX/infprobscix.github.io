---
layout: posts
title:  "Probabilistic PCA"
date:   2020-06-13 00:22:00 +0100
categories: stats python R
entries_layout: grid
---

I've been reading about PPCA, and this post summarizes my understanding of it. I took a lot of this from Pattern Recognition and Machine Learning by Bishop.

The model behind the algorithm is quite simple. We've got $$n$$ observations of a random variable $$X$$ that takes values in $$\mathbb R^m$$. We describe a latent representation $$Z$$ that has dimension $$m$$ or lower as follows. I'll assume that $$X$$ has a zero mean.

$$ X | Z \sim \mathcal N(WZ, \sigma^2 I) $$

$$ Z \sim \mathcal N (0, \mathcal I) $$

We can marginalize out $$Z$$ out:

$$ X \sim \mathcal N(0, WW^T + \sigma^2 I) \;\;\;* $$

Tipping & Bishop (1999) showed that the maximum likelihood solution for W is achieved at:

$$ W_{ML} = U (L - \sigma^2 I)^{1/2} R $$

where $$L$$ is a matrix of (the largest) eigenvalues, $$U$$ is a matrix of corresponding eigenvectors and $$R$$ is an arbitrary orthogonal matrix.

One could work back to $$X$$ using the latent variables using:

$$M = W^T W + \sigma^2 I$$

$$ Z | X \sim \mathcal N(M^{-1} W^T X, \sigma^{-2} M) $$

Here's some Stan code to reproduce the maximum likelihood solution to $$W$$. We recover the correct solution up to rotations, as expected.

<details>
<summary> Stan code for PCA </summary>
 
{%highlight R%}

library(rstan)
library(data.table)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

m = 2; n = 100
Z = matrix(rnorm(m*n), n, m)
W = matrix(rnorm(m*m), m, m)
X = Z %*% W
pca = prcomp(X, center = F, scale = F)

model_string = "
data {
	int m;
	int n;
	matrix[n, m] X;
}
parameters {
	matrix[m, m] W;
}
model {
	vector[m] mu;
	matrix[m, m] L;
	mu = rep_vector(0, m);
	L = cholesky_decompose(W * W');

	for(i in 1:m) {
		W[, i] ~ normal(0, 2);
	}

	for(i in 1:n) {
		X[i, ] ~ multi_normal_cholesky(mu, L);
	}

}
"

model = stan_model(model_code = model_string)
data = list(m = m, n = n, X = X)

# optim_lik = function() optimizing(model, data = data)$par
samples = sampling(model, data = data, chains = 2, iter = 1000)

ml_samples = matrix(extract(samples)$W, 1000, 4)
ml_samples = as.data.table(ml_samples)
names(ml_samples) = c('W11', 'W21', 'W12', 'W22')

eig = eigen(cov(X))
U = eig$vectors
L = eig$values
W_no_R = U %*% diag(sqrt(L))

simulate_rotation = function() {
	theta = runif(1, 0, 2*pi)
	R = matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2, 2)
	W_ML = W_no_R %*% R
	return(W_ML)
}

manual_calc_samples = t(replicate(1000, as.numeric(simulate_rotation())))
manual_calc_samples = as.data.table(manual_calc_samples)
names(manual_calc_samples) = c('MW11', 'MW21', 'MW12', 'MW22')

plot_frame = cbind(ml_samples, manual_calc_samples)

ggplot(plot_frame) +
	geom_hex(aes(W11, W21, fill = 'Stan_W[, 1]'), alpha = 0.3, bins = 100) +
	geom_hex(aes(W12, W22, fill = 'Stan_W[, 2]'), alpha = 0.3, bins = 100) +
	geom_hex(aes(MW11, MW21, fill = 'ML_W[, 1]'), alpha = 0.3, bins = 100) +
	geom_hex(aes(MW12, MW22, fill = 'ML_W[, 2]'), alpha = 0.3, bins = 100) +
	xlim(-3, 3) + ylim(-3, 3) + labs(x = 'x', y = 'y') +
	scale_fill_brewer(palette = 'Spectral') + theme_void()

{% endhighlight %}

</details> <br>

<center> <img src="/images/stpca.png" width="50%"> </center>
