---
layout: posts
title:  "Introduction to the Study of Data"
date:   2018-06-13 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

When dealing with data, I mostly find myself in one of three scenarios:
1. There's no randomness in the data: Situations like this are quite rare, but when they do arise, dealing with them is quite easy; all we'd need to do when confronting such datasets is _**build a set of rules**_ by studying the data.
	*  Historically, this is how a lot of physical laws were discovered, particularly when the phenomenon were so spectacular that they swamped out any little noise present in observations.

2. There's some randomness in the data: There's some uncertainty present in the data, but certain _**frequencies**_ emerge (or are expected to emerge) when studying the data. For example, if I sit still and toss a coin whose mass is distributed evenly / uniformly throughout the coin, I might observe that after many throws, the number of heads comes out to be roughly the same as the number of tails. Where such frequencies seem to exist, probability theory can be used to _**represent**_ those things which seem random but have the peculiar property that their frequencies are roughly predictable. This process of representing random things and trying to understand how they behave is statistics.

3. There's a lot of randomness in the data: It is not at all obvious that there should exist random things with predictable frequencies. If I'm on a moving bus when I'm tossing the coin, I might see that every time the bus banks left, I tend to get more heads than tails and vice versa. So, the random thing here - the number of heads compared with tails - depends on the journey and route I'm taking, and the frequencies of heads is not going to be predictable at all. In cases such as this however, the reason for unpredictability is usually the complexity of the thing we're looking at. If we simplify the problem (i.e. _**consider limiting cases**_), we often end up in scenario 2.

The fact that situation ones are rare and situation threes can be decomposed into situation twos is what makes statistics so important.

Note that, historically, "uncertainty" just means ignorance or complexity. In the coin tossing example, we can predict exactly if the coin ends up being a head or a tail, but often, it's too difficult or unimportant to understand underlying dynamics, and we find ourselves resorting to staistical methods.

Note that, in point 2, I do not claim that the existence of 'stable' or predictable frequencies _defines_ the study of statistics - it is merely an idea and an assumption that we make (and justify) while working with data. One runs into circularity problems really quickly if they try to define statistics in terms of "limiting" frequencies or with other similar ideas.

In fact, this line of reasoning needn't only hold for the idea of frequencies. Consider the plausibility of someone on a court stand being guilty of a crime. Although the jury may never know the truth about the person's extent of culpability, they can reason that: if observables are represented as probabilistic variables, given a causal chain of events, the observables would probably never have happened if not for the person's involement, thus increasing the likelihood of their guilt. This shows that "subjective" views of probability have merit and fundamentally all forms of measuring plausibility may be the same underlying principle.

## Probability Theory

Probability theory gives us a foundation to define and work with random things whose frequencies can be fixed.

(The SEP<sup>[4]</sup>, Jaynes' book<sup>[3]</sup> and similar sources have extremely well-framed presentations on this topic. Below is an extremely short summarization of that information.)

A probability can be interpreted in a number of ways, such as <sup>[4]</sup>:
 * the "fraction" of times an event happens / is "expected" to happen relative to the "fraction" of times it doesn't,
 * the "degree of belief" that a certain event will occur,
 * the "physical disposition" of a situation to yield a particular outcome, etc ...

The key point is that, although all such can be viewed as logical or part of the same overarching _idea_, it's very difficult to _define_ probabilities using such arguments.

There have been two paradigm-defining attempts to define probability.

The first was the description of the laws of probability by A. N. Kolmogorov<sup>[5]</sup>. These laws are as follows<sup>[4, paraphrased by me]</sup>:

Let \\(\Omega\\) be a set of things that can happen and \\(\Sigma\\) is a bunch of interesting questions we can ask. Then, we let \\(\mathcal P\\) be a function called a probability function that takes in interesting questions and spits out a number. \\(\mathcal P\\) obeys the following rules:
 1. \\(\mathcal P(interesting \; question)\\) is either 0 or more than 0.
 2. \\(\mathcal P(\omega)\\) is 1 (so the probability that something - _anything_ - from \\(\Omega\\) happens in 1).
 3. If two things can't both happen, then \\(P(A\;or\;B\;happening) = P(A) + P(B)\\)

Note that this definition doesn't rely on any interpretation. Probability laws are applicable to any mathematical or physical system that obeys these rules. As I understand, there are things in statistical mechanics that obey the rules stated above which have no relation to "probability" in any way whatsoever but the theory still holds.

---
### <span style="color:orange "> Technical Interlude </span>

Feel free to skip this section if you've got no interest in the math. Formally, we need measure theory to properly define probability as Kolmogorov did and although it is so abstract, if you try to define probability, you'll find yourself rediscovering these ideas presented below.

First, let \\(\Omega\\) be a "sample space" - these are events that are interesting (e.g. \\(\Omega = \\{"head", "tail"\\} \\) is a sample space of an experiment which involves tossing a coin once).

Before we get to probability, note that there are a bunch of interesting question we can form with such a set. A probability needs to be _consistent_ throughout all of these interesting questions.

For example, I might ask "What is the probability of heads?" but this can also be answered by asking "What is the probability of tails?" because I can ask "What is the probability that I get either a head or a tail?" and if we set this to 1 (point 2 above), we can say that the "Prob of Head" + "Prob of Tail" = 1 (using point 3) and calculate "Prob of Head" by subtracting "Prob of Tail" from 1.

Note what the probability had to do here: for every event \\(A\\) our probability function knows, the probability function also knows \\(not A\\) means (i.e. it can ). Furthermore, for any two events \\( A \; \& \;B \\) that the probability function can handle, it must also be able to handle \\(A or B\\).

Another example: say that you're tossing a six-sided die (\\(\Omega = \\{1, 2, 3, 4, 5, 6\\} \\)). If I code up a function called probability into a computer and if I define it for events 2, 3 & 6 as \\(\mathcal P(2) = \mathcal P(3) = \mathcal P(6) = 1/6 \\), it should also be able to process questions like \\(\mathcal P(2 or 3) = 2/6 \\) and \\(\mathcal P(not 2) = 5/6 \\).

Mathematically, we say that the set of things \\(\mathcal P\\) needs to be able to work with is _closed under complements_ and also _closed under unions_.

We call such a set of interesting questions a _**\\(\sigma\\)-algebra**_ or a _**\\(\sigma\\)-field**_ that's defined on elements of \\(\Omega\\). The pair of these two sets (\\(\Omega\\), \\(\Sigma\\)) is called a measurable space<sup>[3]</sup> and it is on such sets we define a measure with the normalization, non-negativity and finite additivity conditions and call it a probability measure<sup>[4]</sup>. The triple \\((\Omega, \Sigma, \mathcal P)\\) is called a probability space. The key difference between a measure space and a probability space is that a probability space is endowed with a measure which normalizes to 1<sup>[3]</sup>.

For convenience, we usually work with functions called _**random variables**_ \\(X:\Omega \rightarrow \mathbb R \\) which are defined on measurable spaces. Sometimes, we replace \\(\mathbb R\\) with some other set and call the function a "random element" instead. We do this mainly for convenience e.g. \\( \\{"head", "tail" \\} \rightarrow \\{ 0, 1 \\} \\).

Measurable spaces are a big deal because you'd run into major problems defining measures on non-measurable spaces. Famous (and _mind-blowing_) examples include the [Vitali Set](https://en.wikipedia.org/wiki/Vitali_set) and the [Banach-Tarski Paradox](https://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox). It's profoundly amazing that the construction of these sets relies on the axiom of choice; most mathematicians are comfortable accepting the existence of sets which have no measure than abandoning the axiom of choice. Here's a [cool video](https://www.youtube.com/watch?v=hcRZadc5KpI) on that by PBS Infinite Series & Kelsey Houston-Edwards.

---

The other formalization is due to Cox presented by Jaynes in <sup>[2]</sup> (please refer to this for a full argument). The argument presented is that the Kolmogorov rules can derive by three logical arguments:
 1. "Degrees of plausibility are represented by real numbers".
 2. "Common sense"
 3. "If a conclusion can be reasoned out in more than one way, every possible way should lead to the same result."


Much of probability theory can be derived from the basic axioms put forward by Kolmogorov. This includes the famous Bayes' Theorem:

$$ \mathcal P(B | A) = \frac{\mathcal P(A \; and \; B)}{\mathcal P(A)} $$

This construct also allows us to define _distributions_. For example, a discrete-valued distribution may be defined in terms of the probabilities associated with particular values:

$$ X = \{ z_i \; with \; probability \; p(z_i) $$

Where \\(p\\) is a function that maps integers to probabilities. It  is called a probability mass function. We can extend these to continuous spaces by letting \\(\epsilon p(x)\\) represent the probability of finding a point between \\(x - \epsilon, x + \epsilon\\). This is called a probability density function.

Probability theory gives us very nice interpretations for such probability functions. Particular _forms_ of probability mass / density functions have very particular _meanings_ i.e. they emerge naturally from particular sets of assumptions.

For example:

$$ p(0) = 1 - r \; \leftrightarrow \; p(1) = r $$

... arises natually in experiments with a chance of success equal to r and a chance of failure equal to \\(1 - r\\). This is called a Bernoulli distribution.

$$ p(x) = \lambda e^{-\lambda x} $$

... arises naturally in processes with random waiting times between events where the only known information about the waiting times is that the average waiting time is \\(1/\lambda\\). This is called an exponential distribution.

$$ p(x) = \frac{1}{\sqrt{ 2 \pi }} e^{- \frac{x^2}{2}} $$

... arises naturally in the estimation of averages (shifted and scaled appropriately). This is called a Gaussian or a Normal distribution.

The "r" and the "p" are numbers that produce slightly differently-shaped distributions based on the data set. In fact, estimating these numbers, called parameters, using data is a huge part of statistical inference.

---
### <span style="color:orange "> Technical Interlude </span>

There's an incredible result that shows that many probability distributions are the result of maximizing the entropy (the randomness in some sense) of an arbitrary probability distribution subject to a set of constraints.

If you know the probability of success for a particular process and nothing else, the best you can do is to represent the process using a Bernoulli distribution.

If you know the mean and variance of a particular process and nothing else, the best you can do is to represent it using a normal distribution.

The exponential family is in fact a family of parameterised maximum entropic distributions (i.e. they arise in maximum-entropy conditions with different constraints and have parameters which usually have a very deep physical meaning). It is also the only family of distributions where the dimension of numbers required to estimate them (i.e. the number of sufficient statistics) _**doesn't grow**_ with the number of samples available (this provides further inutition regarding their randomness - they're so random that you can't do any better).

Sufficiency in statistics is the idea that sometimes, you just need a few numbers to completely charecterize a distribution. For example, if you know the sample mean and variance of a normal-looking dataset, you don't need to keep the dataset as the data doesn't give you any more information that's not already present in the mean and variance.

---

Now that we've established a way to deal with things whose frequencies remain roughly stable, we can apply this theory to data.

## Statistics

First, when we obtain data, a foundational step in data processing is to look at so called "[descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics)" which is an attempt to describe data on hand, either through summary statistics or through visualizations:

<img src = "/images/intro_a.png">
<sup> [a] </sup>

The objective of studying data is usually to try and understand:
 * the structure, composition and the interrelationships evident in the data or
 * how certain parts of the data explain / predict other parts of the data

I personally dislike the labelling that's done here ([Generative](https://en.wikipedia.org/wiki/Generative_model) vs [Discriminatory](https://en.wikipedia.org/wiki/Discriminative_model)) but more on that later.

This blog is mainly focused on statistical inference:

<div style="text-align:center"> "Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed." <sup> [1, pg. 4] </sup> </div> <br>

We usually do this by constructing a statistical model, the aim of which is to build a plausible probabilistic representation capable of reproducing (joint) frequencies and other equivalent features evident in the data.

### Non-Parametric Inference

(Write about the bootstrap, how it is such a good estimator of stuff. Also, it can be thought of as sampling n points from the ecdf.)

### Parametric Inference

This usually involves writing down a probabilistic model w.r.t. some parameters and trying to estimate those parameters.

One way to do this is to treat the parameters of a model as random variables and construct probability distributions for them using the Bayes' rule:

$$ p_{\mathcal M} (\theta | {\bf x}) = \frac{p_{\mathcal M} ({\bf x} | \theta) p(\theta)}{p_{\mathcal M} ({\bf x})} \leftrightarrow posterior = \frac{likelihood * prior}{marginal \; likelihood} $$

The likelihood function is also present in frequentist staistics (\\(\theta\\) is estimated as the \\(arg \; max_{\theta} \; ll(\theta)\\) where ll is the log of the likelihood function) - this was Ronald Fischer's great method of parameter estimation.

The likelihood is an amazing, non-trivial construct. It can be defined as the model joint distribution evaluated at the data. In fact, minimizing the KL-divergence is equivalent to maximum likelihood estimation.

## Digression on the Likelihood

The log-likelihood is even more amazing. I can show that the expectation of the log-likelihood is always higher for the true model than any constructed model - which trivially leads to the notion of parameter estimation by maximmizing the log-likelihood (which is also equivalent to minimizing the KL-divergence). The likelihood doesn't share this result (graphically, this can be seen because the mean is not a robust estimator and the likelihood is spiked near 0 with a long tail; the sampling distribution for the log-likelihood on the other hand, looks nice and is asymtotically normal).

Proof of my claim:

_Let \\(Y\\) be a multivariate random variable with mass or density \\(p\\). Further, let_ \\( p_{\mathcal M}\\) _be a hypothesis density such that_ \\(p \sim p_{\mathcal M}\\). _Then:_

$$\mathbb E_p \left( ln \; p(Y) \right) \geq \mathbb E_p \left( ln \; p_{\mathcal M} (Y) \right) $$

_with equality only when_ \\(p = p_{\mathcal M}\\).

***Proof:***

We need to show that the following expression is greater than or equal to 0:

$$ \mathbb E_p \left( ln \; p(Y) \right) - \mathbb E_p \left( ln \; p_{\mathcal M} (Y) \right) $$

$$ = \mathbb E_p \left( ln \; \frac{p(Y)}{p_{\mathcal M} (Y)} \right)$$

By the definition of Kullbeck-Leibler divergence and Gibbs' inequality:

$$ = D_{KL}(Y || \mathcal M) \geq 0 \tag* {$\blacksquare$}$$

Choosing a \\(p_{\mathcal M}\\) to minimize the above is obviously equivalent to maximum likelihood estimation.

It's interesting that although the likelihood turns out to be concave w.r.t. the parameter at most times, it doesn't account for the correctness of a sample; i.e. a likelihood is not maximized at a representative sample of the population. A simple illustration is the fact that the product of exponential likelihoods at 0 will _**always**_ be higher than the product of likelihoods at a random sample _of that exponential_.

The eCDF is the best thing that I know of that compares two distributions. Another is KL Divergence - which is super interesting; I believe that likelihood maximization is a special case of a more general effect. That may or may not be related to the KL divergence (my main concern being that the likelihood isn't maximised at a representative sample from a population unlike the negative of the Kolmogorov-Smirnov metric)

So, I'm not sure how much value the likelihood has in terms of favoring more plausible models over highly complex overfit ones.

Consider a random binomial sample with parameters \\(n = 5, p = 0.5\\) sample being fit with a maximum likelihood binomial and a moment-matched beta binomial. I measure the KL divergence from the empirical mass to both theoretical masses. I then plot the two, KL divergence of the binomial on the x-axis and the beta-binomial on y:

<img style="max-width: 400px; height: auto;" src="/images/intro_c.png">

This clearly shows that the KL-divergence overfits.

More digression...

Andrew Gelman has expressed an opinion that the model evidence / marginal likelihood is not a very useful quantity. I share this opinion mainly because it's so _**incredibly**_ sensitive to the selection of a prior and moreover, it is simply a prior weighted average likelihood - I can't see why it would convey more information than the likelihood. Moreover, information theoretic criteria (which are used in place of the model evidence in some fashion) are just crude bias corrected maximum log-likelihood terms.

## Appendix: Code for Reproducing Diagrams

<details>
<summary> a: Expand R Code Panel </summary>

{%highlight ruby%}

library(ggplot2)
library(gridExtra)

data(iris)
data(faithful)

z <- mvtnorm::rmvnorm(10000, c(0, 0), matrix(c(1, 0.5, 0.5, 1), 2, 2))

df <- data.frame("Location" = qgamma(ppoints(10000), 2, 1),
				 "Spread1" = qnorm(ppoints(10000), 0, 1),
				 "Spread2" = qnorm(ppoints(10000), 0, 2),
				 "Kurt1" = qt(ppoints(10000), 2),
				 "Kurt2" = qt(ppoints(10000), 100),
				 "Norm1" = z[, 1],
				 "Norm2" = z[, 2])

plots <- list(
	ggplot(df, aes(x = Location)) +
		geom_histogram(alpha = 0.7, fill = "green", bins = 100) + xlim(c(0, 6)) +
		geom_vline(xintercept = mean(df$Location), lty = 2, alpha = 0.55) + 
		geom_text(aes(x = mean(df$Location) + 0.1, label = "Mean", y = 200), angle = 90, alpha = 0.7) +
		geom_vline(xintercept = median(df$Location), lty = 2, alpha = 0.7) + 
		geom_text(aes(x = median(df$Location) + 0.1, label = "Median", y = 200), angle = 90, alpha = 0.7) +
		geom_vline(xintercept = 1, lty = 2, alpha = 0.7) +
		geom_text(aes(x = 1.1, label = "Mode", y = 200), angle = 90, alpha = 0.7) +
		geom_curve(aes(x = 5, xend = 6, y = 25, yend = 14), arrow = arrow(length = unit(0.25, units = "cm")), curvature = 0.1) + 
		geom_text(aes(x = 5.5, label = "Skew (+)", y = 23), angle = -20) +
		labs(y = "Frequency", x = "Data"),
	ggplot(df) +
		geom_histogram(aes(x = Spread1), alpha = 0.7, fill = "#72afff", bins = 100) +
		geom_histogram(aes(x = Spread2), alpha = 0.7, fill = "#4f9bff", bins = 100) +
		geom_text(aes(x = 2.5, y = 200, label = "σ = 2")) +
		geom_text(aes(x = 1, y = 600, label = "σ = 1")) +
		labs(x = "Data", y = "Frequency", title = "Differing Standard Deviations"),
	ggplot(df) +
		geom_histogram(aes(x = Kurt2), alpha = 0.5, fill = "red", bins = 100) +
		geom_histogram(aes(x = Kurt1), alpha = 0.5, fill = "orange", bins = 100) +
		geom_text(aes(x = 4, y = 50, label = "κ high")) +
		geom_text(aes(x = 0.75, y = 550, label = "κ low")) +
		xlim(c(-7, 7)) +
		labs(x = "Data", y = "Frequency", title = "Differing Kurtosis"),
	ggplot(df) +
		geom_point(aes(x = Norm1, y = Norm2), alpha = 0.2, color = "dark violet") +
		labs(x = "Data X", y = "Data Y", title = "Correlation"),
	ggplot(iris) +
		geom_point(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +
		labs(title = "Classes") + theme(legend.position = "none"),
	ggplot(faithful) +
		geom_point(aes(x = eruptions, y = waiting), color = "gold") +
		labs(title = "Clusters")
)

grid.arrange(grobs = plots, nrow = 2)

{% endhighlight %}

</details> <br>

<details>
<summary> b: Expand R Code Panel </summary>

{%highlight ruby%}

dbetabin <- Vectorize(function(x, n, a, b) choose(n, x) * beta(x + a, n - x + b) / beta(a, b))

a <- function(x, b) (mean(x) * b / (5 - mean(x)))

b_diff <- function(b, x){
  t <- var(x) - ((5 * a(x, b) * b) * (a(x, b) + b + 5) / (((a(x, b)  + b)^2) * (a(x, b) + b + 1)))
  return(t^2)
}

Dklb <- function(x){
  P <- numeric(5 + 1)
  P[as.numeric(names(table(x))) + 1] <- as.numeric(table(x))
  P <- P/length(x)
  Q <- dbinom(0:5, 5, mean(x)/5)
  log_vals <- log(P/Q)
  log_vals[is.infinite(log_vals)] <- 0
  return(sum(P * log_vals))
}

Dklbb <- function(x){
  P <- numeric(5 + 1)
  P[as.numeric(names(table(x))) + 1] <- as.numeric(table(x))
  P <- P/length(x)
  b <- min(suppressWarnings(optim(1, b_diff, x = x)$par), 300)
  a <- a(x, b)
  Q <- dbetabin(0:5, 5, a, b)
  log_vals <- log(P/Q)
  log_vals[is.infinite(log_vals)] <- 0
  return(sum(P * log_vals))
}

nr <- function(x){
  if(x == 0) return(0.001)
  if(x == 1) return(0.999)
  return(x)
}

KLbin <- numeric(10000)
KLbetabin <- numeric(10000)

for(i in 1:10000){
  s <- rbinom(10, 5, 0.5)
  KLbin[i] <- Dklb(s)
  KLbetabin[i] <- Dklbb(s)
}

library(ggplot2)

ggplot(data.frame("Bin" = KLbin, "BetaBin" = KLbetabin)) +
  geom_point(aes(x = Bin, y = BetaBin), alpha = 0.6, color = "orange") +
  labs(title = "KL-Divergence(Empirical Mass | Best Fit)")

{% endhighlight %}

</details> <br>

<details>
<summary> c: Expand R Code Panel </summary>

{%highlight ruby%}

library(ggplot2)
library(ellipse)
library(analysis)

e <- as.data.frame(ellipse(0.95))

df <- data.frame(x = e$x, y_1 = e$y)
df$poly_x <- rep(c(-1, 0, 1, 0), 25)
df$poly_y <- rep(c(0, -1, 0, 1), 25)
df$line_x <- seq(0.025, 2.5, 0.025)
df$line_y <- sapply(2 * df$line_x - 2.5, function(x) max(x, 0))

ggplot(df) + xlim(c(-5, 5)) + ylim(c(-5, 5)) +
  geom_polygon(aes(x = 0.68*x + 2.5, y = 1.36*y_1 + 2.5), fill = "#aaaaaa") +
  geom_polygon(aes(x = 0.415*x + 2.5, y = 0.83*y_1 + 2.5), fill = "#606060") +
  geom_polygon(aes(x = 0.2*x + 2.5, y = 0.4*y_1 + 2.5), fill = "#111111") +
  geom_polygon(aes(x = poly_x, y = poly_y), fill = "#54b7ff", alpha = 0.6) +
  geom_polygon(aes(x = poly_x*2, y = poly_y*2), fill = "#75c5ff", alpha = 0.4) +
  geom_polygon(aes(x = poly_x*3, y = poly_y*3), fill = "#96d3ff", alpha = 0.2) +
  geom_line(aes(x = line_x, y = line_y), color = "orange", lwd = 1.25, alpha = cos(df$line_x * pi/5)) +
  geom_text(aes(x = 2, y = 4, label = "g(β)")) + geom_text(aes(x = -2, y = 2.5, label = "||h(β)|| < k")) + 
  xlab("β1") + ylab("β2") +
  theme(axis.text.x=element_blank(), axis.text.y=element_blank(),
        axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

{% endhighlight %}

</details> <br>

## Stuff to add later

There's a burn-in period like optimization and then sampling happens (weak correlated markov chain). Rule of three for binomial p with no failure info, KL divergence and likelihood discussion; probabilities may inherently be on a log-scale in the sense that a unit decrease in probability results in a more-than-that-fold increase in the number of samples required., box-cox transform, mcmc has stationary distribution, information theoretc criteria are expectations of log likelihood bias corrected in some way that usually ends up correcting for number of parameters, underidentification: uniform or multimodal posterior (the same if you think about it) or truncation/boundary map/mle, differencing in arima models has the same interpretation as the derivative - 0diff stationary, 1dff constant trend, 2dff - simple time varying trend, etc., likelihood constructed _for_ bayesian stats, gelman: purpose of model checking is to understand in what ways the model doesn't fit data, weakly informative priors are better than non-informative ones as they constrain the space enough so that a sequantial algo will find the distribution quickly, bootstrap is a good estimator of the distribution function, censored likelihoods can be constructed by considering a mixed distrubtion, jacobian transforms and non-problemness of many to one transformations, there are tests more powerful than the ks, priors can reflect sampling uncertainty, ln gamma has a left skew whereas ln normal is symmetric, PCA, ICA, GPs (slow for large datasets), 

A number is an abstract concept; data is physical, lasso and ridge boundary and why, categorical / multinomial / zero-inflated / markov / hidden markov + EM + BM + FB + estimation error for low probabilities / difficulty in identifying trends / semi-markov / wrong models qqplots can be weird - have o choose what to do, fourier estimation for seasonal trends, shrinkage of coefficients to zero e.g. in hierarchical models, identifyability, exchangability: joint distribution remains the same for every permutation of indices, model parameters can be cross correlated when obtaining samples - not to worry about, exponential family is a class of entropy maximising distributions, given constrainsts and are also the only dists where the sufficicnet stats don't increase with number of samples., Inference about the binomial n is ridiculously hard and the MLE is usually useless

Change of measure: you're changing the expected probabalistic behaviour of a system based on the density with which stuff's being changed., Interpolation: spline; polynomial(lagrange polynomials - can have messy oscillation around edges - runge's phenomenon similar to gibbs for fourier series on steps); bicubic / tricubic etc. for higher dimensional data., A fitted GP is the posterior obvs, show math, independance of mean and sd is charecteristic and definitive of normal but ingeneral, cumulants are not independant (although the dependance can be quite weak - from my exps), VC dimension: number of points shattered by an algo (for NN it's very high - but this result is controversial.), Best use bayesian estimation to get uncertainties in larger quantiles, GP sort of procs approach GP (by me), Probabalistic Graphical Models, probabalistic colloquation methods / non intrusive spectral projections: non intrusive because you don't need to mess around with the code before hand. Split up the objective function in terms of a bunch of polynomials that are orthogonal in the expectation sense wrt the underlying probability density so this is hermite if the measure is normal so that 

## Bibliography

1. _**Bayesian Data Analysis**_
  * Gelman A., Carlin J. B., Stern H. S., Dunson D. B., Vehtari A. & Rubin D. B., 2013.

2. _**Probability Theory: The Logic of Science**_
  * Jaynes E. T., 2003.

3. _**A Quick Review of Measure and Integration Theory**_
  * Tao T., 2009. _Accessible at [Terence Tao's Blog](https://terrytao.wordpress.com/2009/01/01/245b-notes-0-a-quick-review-of-measure-and-integration-theory/)._

4. _**Interpretations of Probability**_
  * Hájek, A., The Stanford Encyclopedia of Philosophy (Winter 2012 Edition), Zalta E. N. (ed.). Accessible at the [SEP](https://plato.stanford.edu/archives/win2012/entries/probability-interpret/).

5. _**Foundations of the Theory of Probability** (translated)_
  * Kolmogorov A. N., 1933. _Accessible at [york.ac.uk](https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf)._

6. _**A Review of Probability Theory**_
  * Tao T. 2010. _Accessible at [Terence Tao's Blog](https://terrytao.wordpress.com/2010/01/01/254a-notes-0-a-review-of-probability-theory/)
