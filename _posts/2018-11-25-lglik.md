---
layout: posts
title:  "The Log-Likelihood & Dimensionality"
date:   2018-11-25 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

The log-likelihood function \\(l\\) is a function such that:

$$\mathcal l(\theta; x) = ln \; p_{\mathcal M}(x|\theta)$$

... where p is the _generalized density_ under a model, evaluated on the dataset.

The log-likelihood is an interesting beast. It not only captures the complete model law and the fed dataset (or a sufficient statistic of it), but it also has deep connections with the ideas of information entropy. It's exceptionally well-behaved (numerically) and many modern model comparison methods such as the evidence measure its value or an adjusted (bias? corrected) expectation of it. Typically with i.i.d. experiments, it's approximately normally distributed with an expectation equal to the model entropy.

## Loss Functions

It's quite easy to see why the log-likelihood enforces a model. While optimizing literally anything, one usually has a "loss" containing a function of data and parameters, and another that's a function only of the parameters. Everything else disappears when a derivative is calculated, and this raises an interesting question. If every loss can be written as:

$$ - \mathcal L(\theta; x) = f(x;\theta) + g(\theta) $$

... then what really is the difference between a loss function and a negative log posterior? As far as I've been able to tell, there's no difference, and this result is stunning in practice.

Case in point: _**Regularization**_

Adding a penalty of:

$$k || \underline \beta ||^p$$

... to the regression framework leads to "insignificant" parameters being forced down/close to zero (this actually introduces bias, but biased estimators can have a lower MSE than unbiased estimators). From an optimization perspective, \\(k\\) is a Lagrangian multiplier; for each value it takes, a different curve centered on \\((0, 0, ...)\\) is enforced and a value on the likelihood _g_ is found such that the boundary enforcement is met:

<center> <figure>
	<img style="max-width: 400px; height: auto;" src="/images/l1reg.png"> 
	<img style="max-width: 350px; height: auto;" src="/images/l2reg.png"> 
	<figcaption> p = 1 (LASSO) & p = 2 (Ridge) </figcaption>
</figure> </center>

... but this can be interpret as placing strong Laplacian/Gaussian priors on the parameters and obtaining a MAP solution.

## Model Laws

By constructing logical model laws, one can obtain amazing results with probability theory. Examples:

 1. We can reason about the probability of failure in a repeated Bernoulli experiment even if no failures are observed! Assume that a process represented by \\(X \sim Bin(n, p)\\) produces a set of \\(n\\) successes. We can still construct a posterior by writing out the likelihood of this model. This would obviously include 0 (and the highest mass would be there) but an interval with a 5% error rate would be \\([0, 3/n]\\).

 2. When dealing with censored data, we can re-frame the problem and derive a likelihood. Here's my attempt at that:

	Let a random variable \\(\tau\\) have a cumulative distribution function \\(F_{\tau}\\).

	Now, let \\(X\\) be a random variable such that:

	$$ X = \begin{cases} \tau,  & \text{if $\tau \leq T$} \\
	T, & \text{if $\tau \geq T$} \end{cases}$$

	Then, it's easy to see that the cumulative distribution function of \\(X\\) is:

	$$ F_X(x) = \begin{cases} F_{\tau}(x),  & \text{if $x < T$} \\
	1, & \text{if $x \geq T$} \end{cases}$$

	Let \\(\nu = l + \delta_T\\) be a reference measure that is the sum of the Lebesgue measure \\(l\\) and the Dirac measure \\(\delta_T\\) on the set \\(\{T\}\\). Then, the density on \\(X\\) will be a function \\(h\\) (the Radon-Nikodym derivative) such that:

	$$ F_X(x) = \int_{-\infty}^x h d\nu = \int_{-\infty}^x h(u) du * \mathcal I_{x < T} + h(T) \mathcal I_{x = T} $$

	A function that satisfies the form above is:

	$$h(u) =  \frac{dF_{\tau}}{du}(u) \mathcal I_{u < T} + (1 - F_{\tau}(T)) \mathcal I_{u = T}$$

## Dimensionality

### Volumes in High Dimensions

A lot of probability theory uses the idea of volumes. For example, the probability and expectation are usually defined as:

$$ \mathbb P_{\mu} (\mathcal A) = \int_A d \mu \;\;\; ; \;\;\; \mathbb E_{\mu} (f(X)) = \int_{\chi} f(x) d \mu(x) $$

... where the differential element is usually a volume, \\( d \mu(x) = p(x) dx_1 ... dx_d\\), and p is a density, but volumes are really weird in higher dimensions.

The main idea is that our ideas of _distance_ no longer make sense. Consider the following: I simulated a bunch of random normal points in many dimensions and plotted the average ratio of the distance to nearest point to the distance to farthest point:

<center> <img style="max-width: 500px; height: auto;" src = "/images/hidst.png"> </center>

This ratio tends to one, implying that the nearest and farthest points are very "close" together / all points "look alike".

On the same subject, consider a cube of unit lengths containing a sphere of unit radius in higher dimensions:

<center> <img style="max-width: 400px; height: auto;" src = "/images/cubes.png"> </center>
<center> <img style="max-width: 500px; height: auto;" src = "/images/sphre.png"> </center>

The edges of the cube keep more and more volume away from the sphere. In fact - the volume of a sphere starts decreasing after 5 dimensions.

All of this discussion has a huge implication on computational statistics, as all of our sampling methods rely on a density defined on some volume. Moreover, this has many other areas of effect, for example, in optimization and cluster analysis as points start getting indistinguishable as we move into higher dimensions.

### Effect on Likelihoods

This discussion leads to one very interesting observation: a probability is not a good measure of _typicality_ outside of the first dimension.

Consider the following: the mode of a normal is at zero. In one dimension, if I draw from a normal, the most reasonable guess at what the draw is, is zero. In high dimensions, this isn't the case - if I draw from a 1000-dimensional normal, it's ridiculously unlikely that I'd pick a vector that (0, 0, 0, ...); a more reasonable guess would be "a vector of points where the mean radius is ye much".

A very good analogy I've read on CrossValidated is: let's say that the weights, heights, eyesight, athleticism, intelligence, ... of people is normally distributed. If I picked a random guy on the street and asked him his weight, it would be reasonable to expect that it'd be around the average. Instead, if I asked the person about their height, weight, eyesight ..., it's very unlikely to meet a person who's average at everything. I'm a bit more heavy, averagely tall, more books-ey ... than other people, for example.

Now consider the likelihood of an isolated repeated experiment:

$$L(\theta | x) = \prod_i^n p(x_i|\theta)$$

This is a function of many random variables (which collectively have a dimension n). This will have it's own sampling distribution, and the position where it's maxmized w.r.t. any of the x's isn't a typical value of the likelihood at all.

Example:

{%highlight ruby%}

hist_fancy(replicate(10000, sum(log(dnorm(rnorm(100))))),
           main = "Typical Values of the Loglikelihood", xlab = "ll")

{% endhighlight %}

<center> <img style="max-width: 500px; height: auto;" src = "/images/typlk.png"> </center>

The maximum possible value of the loglikelihood in this case is -91.9, although it's always much lower than that. Loglikelihood values around -140 _for this experiment_ are _typical_.

We call values that lie in this region of typicality the ***typical set*** and it's geometry is expected to be weird. From the other results, we expect it to be a thin layer of mass lying on a shell somewhere in a high dimensional space.

From Andrew Gelman's blog: Interpolating between any two values of the typical set is rather unlikely to result in another typical sample; this is apparently why optimizers and "ensemble methods" (ones which initialize a bunch of points and move them around till they look like a sample from a density) are a bad way to sample and fail in higher dimensional spaces. This is why HMC is said to be so powerful; to sample from an arbitrary density, we can't use grid or ensemble based methods, so a logical alternative is to start with a sample and obtain another sample, which leads logically to the idea of Markov Chain Monte Carlo, at which the Hybrid/Hamiltonian method is rather efficient.

Michael Betancourt has an even more interesting example; instead of sampling, Betancourt first reparameterizes a target density and integrates out all of the nuisance parameters to obtain a marginal distribution of a set of normals w.r.t. their radius. See the booklet "Logic, Probability, and Bayesian Inference" by Betancourt & the Stan Dev Team.

I believe that the sampling distribution of a model's log-likelihood can be very interesting as a comparison / validation tool. I believe that there's a link between MacKay's Bayesian Occam Razor and these sampling distributions, more so than the model evidence (which is basically the prior weighted average of a loglikelihood, at a dataset - literally the prior expectation).

<details>
<summary> R Code for Dimensionality Image </summary>

{%highlight ruby%}

library(mvtnorm); library(ggplot2)

dim_dist <- function(d, samp_size = 10, ...){
	samp <- rmvnorm(samp_size, mean = rep(0, d), sigma = diag(1, d, d))
	dists <- as.matrix(dist(samp, diag = T, upper = T, ...))
	return(mean(sapply(1:samp_size, function(i) min(dists[-i, i])/max(dists[-i, i]))))
}

qplot(x = 1:250,
	  y = sapply(1:250, function(d) dim_dist(d)),
	  main = "Euclidean Distance in High Dimensions",
	  ylab = "Average Ratio of Nearest to Furthest Point",
	  xlab = "Dimension", color = I("dark gray"), ylim = c(0, 1)) +
	  geom_hline(aes(yintercept = 1))

{% endhighlight %}

</details> <br>
