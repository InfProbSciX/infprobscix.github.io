---
layout: posts
title:  "A List of Models"
date:   2018-06-26 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

As stated, all models are simply probabilistic representations of observable phenomenon, and as such, simply describe observations and related uncertainties. If observations are represented in a careful manner, they tend to yield amazingly accurate descriptions of related phenomena (such is the nature of the natural sciences).

Here is an exposition of some commonly used macro-structures and fundamental ideas in statistical modeling.

## Base Distributions

Sometimes, we can only represent something in terms of the frequencies it attains or the uncertainty that's inherent to it. Sometimes, we may only really _need_ to represent something in this way. We model such things using distributions defined on random variables corresponding to those things.

### Parametric Ways

Sometimes, histograms of data really look like a particular distribution. This may be due to deep effects:

1. ***Limiting Cases:*** Averages of things tend to look normal <sup> central limit </sup>, extreme values in bins usually look like they come from what's called the GEV (generalized extreme value) family <sup> FTG </sup> and tails of things usually tend to look like what's called a generalized pareto distribution <sup> PBdH </sup>. There are many theorems in probability theory that state that interesting things happen under certain conditions, when you generate more and more observations, no matter what the starting points were. Sometimes, such limiting theorems give us nice distributions, characterized by a very small set of numbers called parameters that summarize the data. If we're lucky in that these conditions hold, and our histograms look like on of these distributions, we can go ahead and estimate the parameters that characterize our data set.

2. ***Random Cases:*** When only certain facts about a distribution are known, e.g. just the mean and variance or perhaps just that the variable is positive, or just that it's bounded between two numbers, etc., he best we can do is to use a distribution that maximizes the non-informativeness or the randomness given these constraints. This is called the maximum entropic way to choose a distribution. Interestingly, the exponential family is a family of distributions that arises due to a family of such constraints - e.g. known mean and variance \\(\rightarrow\\) normal; positive \\(\rightarrow\\) exponential, etc. It is characterized by:

	$$ p(x | {\bf \theta}) = h(x) exp \left( {\bf \eta(\theta) T}(x) - {\bf A(\theta)} \right) $$

	Interestingly, the exponential family can also be constructed from the idea of sufficiency - a statistic (i.e. a transformation of a set of random variables, usually representing data) is called sufficient if it completely captures the information in a dataset. E.g. the mean and variance are sufficient to estimate the \\(\mu \; \& \; \sigma\\) parameters of a normal distribution assuming that the data came from one. The exponential family ***is the only family*** whose number of sufficient statistics doesn't grow as you get more and more data (read the best you can do is collect this set of numbers - the family is so random, that you can't do better).

	Other non-exponential kind of maxent distributions include the uniform (e.g. the number is random between a and b), bernoulli (the probability of something is p), the binomial (I do n trials of a beroulli experiment), etc.

3. ***Curve fitting:*** When your distribution looks like a distribution you know, you may want to simply approximate the data using that distribution if you do not want to resort to non-parametric ways which would involve keeping all of your data.

Here's a small list of distributions:

| Fully Bounded | Partially Bounded | Unbounded |
| ------------- | ----------------- | --------- |
| Uniform(a, b) | Exponential(l) | Normal(m, s) |
| Bernoulli(p) | Gamma(a, b) | t(n) |
| Binomial(n, p) / Multinomial | Pareto(h, s) | Cauchy(n) |
| Categorical(p) | Poisson(m) | --------- |

Distributions can of course be compounded, zero-inflated, etc.


### Non-Parametric Ways

In probability theory, a joint distribution of all the observables ***completely*** describes all the data that you've got, but the joint density, the moment generating function of a distribution, the characteristic function, the cumulant generating function and the probability generating function are all equivalent to it (i.e. if I gave you a joint distribution, you'd be able to write a unique moment generating function).

The joint distribution, in other words, the joint cumulative distribution function is a non-decreasing function that starts at zero and end at one, which curves more in an area if that area has a lot of probability associated to it. It is a function that tells you the probability that a random variable is less than a number.

The probability mass or density is a function that tells you how much probability is associated to an area. The mass is straightforward (e.g. probability of 3 = 0.5) and the density is a generalization of this (e.g. probability of getting a value between \\(3 - \epsilon \; \& \; 3 + \epsilon \approx p \epsilon\\)).

The generating functions are just clever functions that spit out moments (mean of \\(x\\), mean of \\(x^2\\), mean of \\(x^3\\)) or cumulants (e.g. mean, variance, skewness, kurtosis, ...) or probabilities, or other such numbers when you do some mathematical manipulations on them (e.g. differentiate and set to zero).

If one doesn't desire to make a distribution assumption as in the parametric case, one can simply look at the distribution implied by the data, by looking at the _empirical_ cumulative distribution function (which is a very strong estimator of the "true" distribution if it exists) or the _empirical_ density by doing what's called kernel density estimation.

Note that sometimes, all one's interested in is a transformation of a data set, and one needn't make distribution assumptions to estimate a distribution for the transformation, one simply needs to ***bootstrap*** the available data (i.e. pick the same number of samples as in the data, from the data, with replacement - i.e. sample multiple datasets from the eCDF). This can be shown to be a much better estimator of a transformation than many limiting theorems simply because the eCDF of the data is a much better estimator of the true distribution w.r.t. than a limiting theorem.

## Function Fitting

<details>
<summary> Code </summary>

{%highlight ruby%}

// code

{% endhighlight %}

</details> <br>

## Bibliography

1. _**Book**_
  * Author A., YYYY.
