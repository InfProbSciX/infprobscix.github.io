---
layout: posts
title:  "A List of Models"
date:   2018-06-26 00:00:00 +0100
categories: stats
entries_layout: grid
---

## Introduction

As stated, all models are simply probabilistic representations of observable phenomenon, and as such, simply describe observations and related uncertainties. If observations are represented in a careful manner, they tend to yield amazingly accurate descriptions of related phenomena (such is the nature of the natural sciences).

Here is an exposition of some commonly used macro-structures and fundamental ideas in statistical modeling.

(Pictures & code will come in due time!)

## Base Distributions

Sometimes, we can only represent something in terms of the frequencies it attains or the uncertainty that's inherent to it. Sometimes, we may only really _need_ to represent something in this way. We model such things using distributions defined on random variables corresponding to those things.

### Parametric Ways

Sometimes, histograms of data really look like a particular distribution. This may be due to deep effects:

1. ***Limiting Cases:*** Averages of things tend to look normal <sup> central limit </sup>, extreme values in bins usually look like they come from what's called the GEV (generalized extreme value) family <sup> FTG </sup> and tails of things usually tend to look like what's called a generalized pareto distribution <sup> PBdH </sup>. There are many theorems in probability theory that state that interesting things happen under certain conditions, when you generate more and more observations, no matter what the starting points were. Sometimes, such limiting theorems give us nice distributions, characterized by a very small set of numbers called parameters that summarize the data. If we're lucky in that these conditions hold, and our histograms look like on of these distributions, we can go ahead and estimate the parameters that characterize our data set.

2. ***Random Cases:*** When only certain facts about a distribution are known, e.g. just the mean and variance or perhaps just that the variable is positive, or just that it's bounded between two numbers, etc., he best we can do is to use a distribution that maximizes the non-informativeness or the randomness given these constraints. This is called the maximum entropic way to choose a distribution. Interestingly, the exponential family is a family of distributions that arises due to a family of such constraints - e.g. known mean and variance \\(\rightarrow\\) normal; positive \\(\rightarrow\\) exponential, etc. It is characterized by:

	$$ p(x | {\bf \theta}) = h(x) exp \left( {\bf \eta(\theta) T}(x) - {\bf A(\theta)} \right) $$

	Interestingly, the exponential family can also be constructed from the idea of sufficiency - a statistic (i.e. a transformation of a set of random variables, usually representing data) is called sufficient if it completely captures the information in a dataset. E.g. the mean and variance are sufficient to estimate the \\(\mu \; \& \; \sigma\\) parameters of a normal distribution assuming that the data came from one. The exponential family ***is the only family*** whose number of sufficient statistics doesn't grow as you get more and more data (read the best you can do is collect this set of numbers - the family is so random, that you can't do better).

	Other non-exponential kind of maxent distributions include the uniform (e.g. the number is random between a and b), bernoulli (the probability of something is p), the binomial (I do n trials of a beroulli experiment), etc.

3. ***Curve fitting:*** When your distribution looks like a distribution you know, you may want to simply approximate the data using that distribution if you do not want to resort to non-parametric ways which would involve keeping all of your data.

Here's a small list of distributions:

| Fully Bounded | Partially Bounded | Unbounded |
| ------------- | ----------------- | --------- |
| Uniform(a, b) | Exponential(l) | Normal(m, s) |
| Bernoulli(p) | Gamma(a, b) | t(n) |
| Binomial(n, p) / Multinomial | Pareto(h, s) | Cauchy(n) |
| Categorical(p) | Poisson(m) | --------- |

Distributions can of course be compounded, zero-inflated, etc. One can also mix and match distributions by using mixture distributions (\\(e.g. \; f(x) = p_1 f_1 (x) + p_2 f_2 (x)\\)).


### Non-Parametric Ways

In probability theory, a joint distribution of all the observables ***completely*** describes all the data that you've got, but the joint density, the moment generating function of a distribution, the characteristic function, the cumulant generating function and the probability generating function are all equivalent to it (i.e. if I gave you a joint distribution, you'd be able to write a unique moment generating function).

The joint distribution, in other words, the joint cumulative distribution function is a non-decreasing function that starts at zero and end at one, which curves more in an area if that area has a lot of probability associated to it. It is a function that tells you the probability that a random variable is less than a number.

The probability mass or density is a function that tells you how much probability is associated to an area. The mass is straightforward (e.g. probability of 3 = 0.5) and the density is a generalization of this (e.g. probability of getting a value between \\(3 - \epsilon \; \& \; 3 + \epsilon \approx p \epsilon\\)).

The generating functions are just clever functions that spit out moments (mean of \\(x\\), mean of \\(x^2\\), mean of \\(x^3\\)) or cumulants (e.g. mean, variance, skewness, kurtosis, ...) or probabilities, or other such numbers when you do some mathematical manipulations on them (e.g. differentiate and set to zero).

If one doesn't desire to make a distribution assumption as in the parametric case, one can simply look at the distribution implied by the data, by looking at the _empirical_ cumulative distribution function (which is a very strong estimator of the "true" distribution if it exists) or the _empirical_ density by doing what's called kernel density estimation.

Note that sometimes, all one's interested in is a transformation of a data set, and one needn't make distribution assumptions to estimate a distribution for the transformation, one simply needs to ***bootstrap*** the available data (i.e. pick the same number of samples as in the data, from the data, with replacement - i.e. sample multiple datasets from the eCDF). This can be shown to be a much better estimator of a transformation than many limiting theorems simply because the eCDF of the data is a much better estimator of the true distribution w.r.t. than a limiting theorem.

## Classical Function Fitting (& Regression)

### Linear Models

We try to understand how an observable Y changes as a set of explanatory observables X change.

$$ Y = \theta^T {\bf X} + {\bf \epsilon}; \; \forall i: \; \epsilon_i \sim \mathcal N(0, \sigma^2) $$

Note that one can transform a set of observables \\(\bf X'\\) into the set **X** by any multivariate transformation.

Assumptions include:

* **X**s aren't correlated with each other (if they are, look for spurious sources of correlation).
* **X**s aren't correlated with the \\(\epsilon_i\\).

One can obtain "robust regressors" by changing the normal to a Laplace or a student _t_ t make the parameters less affected by errors (as both mentioned distributions have fatter tails than the normal).

Robust Regression:

$$ Y = \theta^T {\bf X} + {\bf \epsilon}; \; \forall i: \; \epsilon_i \sim \mathcal Laplace(0, b) \; or \; \mathcal t (0, s, \nu)$$

One can obtain the regularized versions of regression by setting Laplace or Normal priors on the parameters \\(\bf \beta\\) - or equivalently:
Add a penalty to the sum of squares such that the log-likelihoods attain a particular quantile (this can be done by adding \\(||\beta||_ 1 \; or \; ||\beta||_ 2\\) respectively).

LASSO / Ridge:

$$ Y = \theta^T {\bf X} + {\bf \epsilon}; \; \forall j: \; \beta_j \sim \mathcal Laplace \; or \; N(0, \tau) $$

There's also apparently a connection between regularized regression and PCA.

### Generalized Linear Models

The goal is very similar to the case above; we model:

$$ \mathbb E({\bf Y} | {\bf X}) = f(\theta^T {\bf X}); \; Y \sim ArbDist $$

The function \\(f^{-1}\\) is called the link function. Special links include:

$$ Logit: f(X) = \frac{1}{1 + exp \left( - \theta^T {\bf X} \right)}; Y \sim Bernoulli \; (Logistic \; Reg.)$$

$$ Log: f(X) = exp \left( \theta^T {\bf X} \right); Y \sim Poisson $$

$$ Softmax: p_i(X) = f(X) = \frac{exp(\beta_i^T {\bf X})}{1 + \sum_{j}^{k-1} exp(\beta_j^T {\bf X})}; Y \sim Categorical \; (Multinomial \; Reg.)$$

If some of the variables in **X** are constant across group (e.g. indicative group factors), this type of regression may be interpret as a mixed effects model and is called a Generalized Linear Mixed Model.

### Generalized Additive Models

We simply replace the linear predictor with a bunch of functions from the GLM:

$$ \mathbb E({\bf Y} | {\bf X}) = f \left( \sum_i g_{i;\beta_i}({\bf X}) \right) $$

... where _g_ may be parametric (e.g. some function indexed by a set of parameters) or not (e.g. splines).

#### Note on Seasonality Modeling

In probability & time series theory, there is a tool called a spectrogram. As a sine wave may be interpret as a representation of the size of the wave over time, a spectrogram is a representation of the wave's power in terms of its frequencies.

For a smooth estimation of seasonality, one could fit a series that looks like:

$$ s_t = \sum_i^n m_i sin(2 \pi f_i t + \psi_i) $$

... where the frequencies \\(f_i\\) are read off directly from the spectrogram. Noe that it may not be trivially easy to pick the frequencies or the right functional form for the periodic curve as the spectrogram can include multiples of the same frequency (e.g. if a wave is periodic with a frequency f, it will also be periodic with frequency 0.5f and so on,  but there would be only half as many of these waves so the spectrogram power would be slightly smaller). One way to combat this issue is to set a seasonal period and look for frequencies strictly higher than the one corresponding to the set period.

The only parameters which need estimation are \\(m_i \; \& \; \psi_i\\).

<center> ... </center>

<br>

Note how similar these models are - the point of this section is not to intimidate the reader with notation and terminology, but to show that most of these models differ very slightly in their core idea and probabilistic representation.

## Other Function Fitters (& Regressors & Smoothers & Filters)

The following section details regressors that are way more useful for trend estimation than linear models (as linear models are usually constructed when a clear explanatory chain between X & Y is sought).

### The Spline

### The Gaussian Process

 
<details>
<summary> Code </summary>

{%highlight ruby%}

// code

{% endhighlight %}

</details> <br>

## Bibliography

1. _**Machine Learning: A Probabilistic Perspective**_
  * Murphy K., 2012.
2. _A Discussion on Endogeneity [on CrossValidated](https://stats.stackexchange.com/questions/59588/what-do-endogeneity-and-exogeneity-mean-substantively)_.
3. _[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)_
 * Hastie, T., Tibshirani, R. & Friedman J. 2017.